{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "3c532bfb-c54e-4bd6-ae99-da41acde9173",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.7.1.\n",
                        "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "import weaviate\n",
                "client = weaviate.Client(\n",
                "    url=\"http://192.168.2.186:8080\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "6778ad4d-0586-4315-b296-c8d300f93f0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import openai\n",
                "import os\n",
                "def get_embedding_openai(text, engine=\"text-embedding-3-large\") : \n",
                "    Openai_API_KEY = \"\"\n",
                "    os.environ[\"OPENAI_API_KEY\"] =  Openai_API_KEY\n",
                "    openai.api_key =os.getenv(\"OPENAI_API_KEY\")\n",
                "\n",
                "    # res = openai.Embedding.create(input=text,engine=engine)['data'][0]['embedding']\n",
                "    from openai import OpenAI\n",
                "    embedding_client = OpenAI()\n",
                "    res= embedding_client.embeddings.create(input = text, model=engine).data[0].embedding\n",
                "    return res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "ed7b3d6d-22fa-425f-ab11-587ec236b24a",
            "metadata": {},
            "outputs": [],
            "source": [
                "known_class=[\"b_with_title\",\"law\"]\n",
                "def set_db_client():\n",
                "    client = weaviate.Client(\n",
                "        url=\"http://192.168.2.186:8080\"\n",
                "    )\n",
                "    return client\n",
                "def db_class_sync_check():\n",
                "    global client\n",
                "    global known_class\n",
                "    print(\"--- Synchronizing vecotr DB ---\")\n",
                "    db_classes=list(map(lambda x : x[\"class\"].lower(), client.schema.get()[\"classes\"]))\n",
                "    for dc in db_classes:\n",
                "        if dc not in known_class:\n",
                "            print(f\"--- delte db class {dc} ---\")\n",
                "            client.schema.delete_class(dc)\n",
                "            \n",
                "def db_class_check():\n",
                "    global client\n",
                "    print(\"--- DB class check ---\")\n",
                "    db_classes=list(map(lambda x : x[\"class\"].lower(), client.schema.get()[\"classes\"]))\n",
                "    return db_classes\n",
                "\n",
                "def del_weaviate_class(class_name):\n",
                "    global client\n",
                "    print(f\"--- delte db class {class_name} ---\")\n",
                "    client.schema.delete_class(class_name)\n",
                "        \n",
                "def create_weaviate(class_name):\n",
                "    global client\n",
                "    print(\"--- Create DB ---\")\n",
                "    class_obj = {\n",
                "        \"class\": class_name,\n",
                "        \"vectorizer\": \"none\",\n",
                "    }\n",
                "    client.schema.create_class(class_obj)\n",
                "\n",
                "def save_weaviate(class_name,vectorizing_element,chunks):\n",
                "    global client\n",
                "    client.batch.configure(batch_size=100)\n",
                "    print(f\"--- Save DB (data size: {len(chunks)}) ---\")\n",
                "    with client.batch as batch:\n",
                "        for i, chunk in enumerate(chunks):\n",
                "            if i > 185000:\n",
                "                if i%1000==0:print(f\"{i}/{len(chunks)}\")\n",
                "                vector = get_embedding_openai(chunk[vectorizing_element])\n",
                "                batch.add_data_object(data_object=chunk, class_name=class_name, vector=vector)\n",
                "    print(\"--- Save DB DONE ---\")\n",
                "\n",
                "import json\n",
                "def load_json(data_path):\n",
                "    print(\"--- Load Data ---\")\n",
                "    with open(data_path) as f:\n",
                "        data = json.load(f)\n",
                "    return data\n",
                "\n",
                "def db_processing(class_name,data_path,vectorizing_element):\n",
                "    db_class_sync_check()\n",
                "    db_class_list=db_class_check()\n",
                "    if class_name in db_class_list: del_weaviate_class(class_name)\n",
                "    create_weaviate(class_name)\n",
                "    chunks=load_json(data_path)\n",
                "    save_weaviate(class_name,vectorizing_element,chunks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "8cd39101-cc72-4c15-82f4-da21cb0c65d7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Synchronizing vecotr DB ---\n",
                        "--- DB class check ---\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "['b_with_title']"
                        ]
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "db_class_sync_check()\n",
                "db_class_check()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "723cc30e-a220-4434-8ebf-de3f083bc5cf",
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Synchronizing vecotr DB ---\n",
                        "--- DB class check ---\n",
                        "--- delte db class law ---\n",
                        "--- Create DB ---\n",
                        "--- Load Data ---\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdb_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlaw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/.gen/Crawler/Law/law_final_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlaw_content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[40], line 61\u001b[0m, in \u001b[0;36mdb_processing\u001b[0;34m(class_name, data_path, vectorizing_element)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_name \u001b[38;5;129;01min\u001b[39;00m db_class_list: del_weaviate_class(class_name)\n\u001b[1;32m     60\u001b[0m create_weaviate(class_name)\n\u001b[0;32m---> 61\u001b[0m chunks\u001b[38;5;241m=\u001b[39m\u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m save_weaviate(class_name,vectorizing_element,chunks)\n",
                        "Cell \u001b[0;32mIn[40], line 53\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Load Data ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
                        "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "db_processing(\"law\",\"/workspace/.gen/Crawler/Law/law_final_data.json\",\"law_content\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "9dee16e6-40bf-44da-a6d7-6f05972d306a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- delte db class law ---\n"
                    ]
                }
            ],
            "source": [
                "del_weaviate_class(\"law\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2578661d-e9f4-477d-b42b-5f8e547c3041",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d94d8049-e804-49df-a5f3-2ca7bdb3c60b",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b9ad0438-8c89-4398-a7a0-e74fae5a7f44",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b016823-c3d9-4017-afd1-09441ac786bc",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "c4df6314-7ab1-45ca-af0e-54d1db988faf",
            "metadata": {},
            "outputs": [],
            "source": [
                "property_list = list(map(lambda x: x[\"name\"], client.schema.get(\"b_with_title\")['properties']))\n",
                "result = client.query.get(\"b_with_title\", property_list).do()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db1a5cbf-1de9-472d-8839-49eefef89b66",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2e62daa9-1dd2-4056-85f5-71b8ba21d3b8",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b6cfc7f-d029-4cf3-bd2f-a82733b2cfa5",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7774a46a-5c89-4180-af14-88088431c3e3",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff9ff499-260e-4d96-879e-784fcb7376ae",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d81fb0f-f95b-4ee6-ab4e-3235b1acc0ff",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "595f074c-649f-45d9-8151-32c04051e671",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "4a86dcd6-dc05-409c-994c-dcc719f5503e",
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'data': {'Get': {'B_with_title': [{'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': '예문 3번의 경우, that the parents love라는 관계절을 고려하여 주어와 동사의 수일치를 이룰 수 있는지를 볼 수 있다. 예문 4번의 경우는 anaphora의 distribution에 대해 알아보는 문장으로, that the architects like가 manager에 IP(Inflection Phrase), TP(tense phrase) 혹은 CP(Complementizer Phrase)로 귀속될 수 있기 때문에 architects는 뒤에 나오는 anaphora를 C-commend(성분통어)하지 못한다. 따라서 binding theory에 따라 architects와 수가 맞더라도 themselves가 올 경우 비문이 된다. 예문 5번은 NPI(부정극어)에 대한 distribution을 알아보는 문장이다. 이러한 문장은 모두 2009년에 사람을 대상으로 진행된 실험에서 가져온 문장으로, tagging이 전혀 되지 않은 raw 코퍼스에서는 찾아보기 힘들다. 이에 따라 tagging되지 않은 코퍼스로 훈련한 RNN모델에 위와 같은 문장으로 test를 돌려본 결과, 정확도가 극적으로 떨어지는 것을 확인할 수 있었다. 따라서 이러한 실험을 통해 모델의 취약점을 찾아 보완할 수 있다. 하지만 이와 같은 실험을 진행할 때 local heuristics를 방지해야 한다. 즉, 모델이 잔머리 굴리는 것을 방지해야 한다. (6) a. The little boy who is crying is hurt. b. Is the little boy who is crying hurt? c. *Is the little boy who crying is hurt?'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': '이 실험에서는 예문10과 같은 평서문이 주어지지 않은 상태에서 11번을 형성하는지에 대해 실험하였다. 그 결과 몇몇 network가 structural components를 잘 학습하였는데, 이 network들과 어린 아이들이 보이는 오류 양상에는 차이가 있는 것을 발견하였다. 따라서 이러한 연구를 통해 neural network와 인간의 학습 방식은 다르다는 것을 확인할 수 있었고, 그에 따라 neural network에는 syntactic constraint가 딱히 필요하지 않다는 것도 알 수 있었다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/',\n",
                            "     'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
                            "     'text': 'Table 8에 나타난 실험 결과를 보면 RAND-TAPT의 성능이 가장 낮고, kNN-TAPT에서는 k가 클수록 높은 성능을 보이는 것을 확인할 수 있다. 4.3 Computational Requirements Table 9는 BIOMED domain의 RTC-500에 대해 지금까지 수행한 모든 adaptation technique의 computational requirements이다. Reference Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith.”Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks,”ACL. 2020 × Search'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
                            "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
                            "     'text': '아래 그림은 Transformer의 전체 구조이다. 해당 그림을 보면 encoder에 input(source language)이, decoder에는 right shifted된(start token이 있는) output(target language)이 들어가 이를 통해 최종적으로 output probability(다음에 오게 될 단어에 대한 예측 확률)가 도출되는 것을 볼 수 있다:'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/',\n",
                            "     'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
                            "     'text': '그리고 token들의 순서 정보를 반영해주기 위해 position embedding을 해준다. 이렇게 총3단계의 embedding과정을 거친 input이 transformer의 encoder부분과 같은 구조를 가진 모델에 들어가게 된다. Pre-training 앞서 BERT는 pre-train을 위한 semi-supervised step과 특정 task 수행을 위한 supervised step으로 나뉘어 진행되는 것을 언급한 바 있다. Pre-training의 목적은 학습된 모델이 다른 특정 task들을 수행할 때 이미 학습된 모델을 사용함으로써 모델의성능을 높이는 것이다. 따라서 pre-trained된 모델은 다른 과제 수행 시 성능 향상에 도움이 될 만한 과제를 학습하는 것이 좋다. BERT에서는 pre-training task로 masking된 단어를 예측하는 과제와 다음 문장을 예측하는 과제를 동시에 수행한다. Task1 : Masked Language Model(MLM)'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/03/04/similar_search/',\n",
                            "     'source_title': 'RAG : Similarity search (Keyword Search, Semantic Search, Hybrid Search)\\n      04 Mar 2024',\n",
                            "     'text': \"output: [{'rank': 4, 'sentence': '최근 생성형 모델과 함께 사용되는 RAG의 retrieval 단계에서는 qeury와 유사한 chunk를 찾는 것이 매우 중요하다. 검색된 chunk가 모델에 참고 문서로 입력되기 때문에 유사도 검색 결과가 최종 결과에 큰 영향을 미친다.'}, {'rank': 1, 'sentence': 'Semantic Search는 text를 모델을 통해 embedding시킨 후 embedding vector들의 거리를 통해 유사도를 검색하는 방법이다.'}, {'rank': 3, 'sentence': 'BM25는 TF-IDF 알고리즘을 기반으로 한 키워드 검색 알고리즘이다. 매우 오래된 알고리즘이고, 이를 기반으로 한 여러 variation들이 제안되었지만 keyword search에 있어서 아직까지는 클래식이 베스트이다.'}, {'rank': 2, 'sentence': '사용자의 질문이 명확하지 않은 경우, similarity search 과정에서 오류가 발생할 수 있다.'}] (Hybrid Search의 단점은 하나의 알고리즘만 실행하는 것보다 더 많은 컴퓨팅 자원이 필요하다는 것이다.) × Search\"},\n",
                            "    {'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/',\n",
                            "     'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
                            "     'text': \"Alpaca data Alpaca data to Dolly data: instruction → instruction output→ response input → context category → open_qa Alpaca data 예시(1세트) → instruction, input, output Alpaca data sample: {'instruction': '다음 국가와 관련된 역사적 사건 3가지를 나열하십시오.', 'input': '캐나다', 'output': '치푸아츠(Fort Chippewyan)는 캐나다 알버타주의 따뜻한 해안에 위치한 미국 원주민 지역에 근접한 유일한 유럽인 거주지입니다. 캐나다 프랑스 식민지로 건설되었으며, 나스강을 따라 서부 지역으로 확장된 유일한 유럽인 거주지입니다. 1778 년 새로 지어진 건물에 현재는 휴게소가 있습니다. 시티오브 에드먼튼에서 선퍼드 언덕스를 향해 북쪽으로 4.5시간 떨어진 곳에 있습니다.'} Korquad 2.0 Korquad 2.0 data to Dolly data: Q → instruction A → response context → context category → open_qa\"},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': '사람과 최대한 유사한 언어 기능을 구사하는 기계를 만드는게 목표니까… 그 이후 더 개선된 모델들이 계속 나오는 중이다.)'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/',\n",
                            "     'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
                            "     'text': '현재 NLP 응용분야로는 맞춤법 검사, 키워드 검색, 동의어 탐색, 정보 추출, 문서 분류, 기계번역, 질의응답 등이 있다. What’s Natural Language? 자연어란 인간이 일상 생활에서 사용하는 언어를 가리킨다. 인간의 언어는 인간이 의사소통에서 사용하는 도구로 표현될 수 있는데 언어라는 도구를 사용하여 인간은 서로 신호를 보내고 받는 행위를 한다. 일반적으로 언어라 통칭되는 것은 linguistic entity가 아니라 단순 symbol이다. 즉, 하나의 단어는 기의(시니피에)과 기표(시니피앙)을 mapping한 것이다. 예를 들어 “책상”이라는 단어는 해당 물체를 가리키는 단어이다. Symbol은 여러 방식으로 표현될 수 있는데 어떠한 방식으로든 언어는 연속적인 신호들(continuous signals)로 뇌에 전달된다. (기호학)'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/',\n",
                            "     'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
                            "     'text': 'Post-Retrieval 단계에서는 검색된 문서들을 정제하여 답변 생성 품질을 향상시키기 위한 처리들을 한다. 1) Re-Ranking 검색 결과에 대해 유사도 점수 등을 기준으로 chunk를 정렬하여 query와 관련성 높은 문서를 강조하기 위한 처리이다. Re-Ranking model은 query와 context를 input으로 받아 유사도를 측정한다. 2) Prompt compression 관련 없는 context는 제거하고 중요한 context는 강조하여 전체 prompt 길이를 줄인다. 이 과정을 통해 LLM을 중요한 정보만 입력 받아 정확한 답변을 생성할 가능성이 높아지고, LLM의 입력이 줄어들어 추론 속도가 감소할 수 있다. 3) Filtering 키워드나 metadata를 기반으로 검색된 chunk를 filtering하는 단계로, 관련 없는 문서가 검색되었을 때 해당 문서를 제거하기 위한 과정이다. 4) Decomposition multi-query과 같이 query를 여러 버전으로 생성 혹은 분리한 후 각 qeury들에 대한 관련 문서를 검색하여 LLM을 통해 각각의 답을 생성하여 각 qa set을 최종적으로 LLM에 입력하는 방법이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/',\n",
                            "     'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
                            "     'text': '사용할 role scheme을 미리 선택하고 선택한 role scheme에 따라 input token을 구성한다. TPDN의 representation이 신경망 모델(BERT)에 의해 학습된 representation에 근사하게 훈련이 되면 해당 role scheme이 compositionality를 조금 더 잘 명시할 수 있다는 개념을 바탕으로 실험이 진행된다. 7.2 Result 위 표는 사전 학습된 BERT의 representation과 TPDN의 representation에 대한 Mean squared error(평균제곱오차값)을 구한 것이다. 대부분의 layer에서 TPDN이 BERT에 근사한 결과값을 내는 것을 볼 수 있다. left-to-right이나 right-to-left와 같이 단순한 정보처리 방식은 하위 layer에서 좋은 성능을 보이는 반면 tree structure와 같이 syntactic하고 복잡한 정보 처리 에 있어서는 상위 layer의 성능이 좋은 것을 확인할 수 있다. 7.3 Additional experiment 위와 같은 연구결과를 바탕으로 Raganato and Tiedemann(2018)이 진행한 self-attention weight로부터 제고된 dependency tree에 대한 연구를 추가적으로 진행한다. 이 실험은 모델이 문장 내 단어 간의 dependency관계를 파악할 수 있는지를 확인한다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/',\n",
                            "     'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
                            "     'text': 'findall 정규표현식에 해당하는 모든 문자열을 찾아서 리스트에 담아 반환한다. print(re.findall(\\'[\\\\d]\\',s1)) # s1에서 숫자를 다 뽑아내라 # 해당 문자열에서 문자나 숫자가 아닌 걸 다 뽑아내라 print(re.findall(\\'[\\\\W]\\',\\'1#sjspoen@%$^%&95736\\')) [\\'1\\', \\'5\\', \\'3\\', \\'2\\', \\'2\\'] [\\'#\\', \\'@\\', \\'%\\', \\'$\\', \\'^\\', \\'%\\', \\'&\\'] finditer find한 결과를 iterator객체로 반환하는 함수이다. iterator객체에 담겨있기 때문에 하나씩 빼주는 작업을 통해 결과를 하나씩 확인할 수 있다. iter1 = re.finditer(\\'[\\\\d]\\',s1) # s1에서 숫자를 다 뽑아내라 print(\"iter1: \", iter1) # iter1을 출력해보면 iterator객체에 담겨있다고 나온다 for i in iter1: # iter1에 한번씩 방문하여 들어있는 걸 꺼내서 출력하자 print(\"iter1에서 찾은 결과: \",i) iter1: <callable_iterator object at 0x0000021C924383C8> iter1에서 찾은 결과: <re.Match object; span=(4, 5), match=\\'1\\'> iter1에서 찾은 결과: <re.Match object; span=(7, 8), match=\\'5\\'> iter1에서 찾은 결과: <re.Match object; span=(11, 12), match=\\'3\\'> iter1에서 찾은 결과: <re.Match object; span=(17, 18), match=\\'2\\'> iter1에서 찾은 결과: <re.Match object; span=(21, 22), match=\\'2\\'> iter2 = re.finditer(\\'[\\\\W]\\', \\'nkisdb3845$$^%&#\\') print(\"iter2: \", iter2) for i in iter2: print(i)'},\n",
                            "    {'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/',\n",
                            "     'source_title': '한국어 PDF Parser / PDF OCR (layout detection + text extraction)\\n      02 Feb 2024',\n",
                            "     'text': '# Import necessary libraries from PIL import Image import requests from transformers import AutoModelForCausalLM from transformers import AutoProcessor from transformers import BitsAndBytesConfig import torch from IPython.display import display import time, tqdm # Define model ID model_id = \"microsoft/Phi-3-vision-128k-instruct\" # Load processor processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True ) # Define BitsAndBytes configuration for 4-bit quantization nf4_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16, ) # Load model with 4-bit quantization and map to CUDA model = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"cuda:1\", trust_remote_code=True, torch_dtype=\"auto\", quantization_config=nf4_config, ) def model_inference(messages, path_image): start_time = time.time() image = Image.open(path_image) # Prepare prompt with image token prompt = processor.tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) # Process prompt and image for model input inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:1\") # Generate text response using model generate_ids = model.generate( **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=500, do_sample=False, ) # Remove input tokens from generated response generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :] # Decode generated IDs to text response = processor.batch_decode( generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False )[0] display(image) end_time = time.time() print(\"Inference time: {}\".format(end_time - start_time)) # Print the generated response print(response) prompt_cie_front = [{\"role\": \"user\", \"content\": \"<|image_1|>\\\\nOCR the text of the image. Most of the text is in Korean.\"}] path_image = \"./1.png\" model_inference(prompt_cie_front, path_image) microsoft/Phi-3-vision-128k-instruct OCR 결과 2.2 PdfReader + ocrmypdf'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/',\n",
                            "     'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
                            "     'text': '이전 게시물에서는 seq2seq모델에 대해 살펴보았다. 이제 seq2seq모델의 한계점 중 하나인 장기 기억력 문제를 해결하기 위해 고안된 Attention Mechanism을 seq2seq에 추가한 방식을 다룬 논문 ‘NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE‘에 대해 설명하도록 하겠다. Attention Mechanism은 현재 다양한 deep learning model에 활용되고 있지만 기계번역을 위한 seq2seq에 가장 처음 도입되어 소개되었다. 기존 seq2seq모델에서는 Encoder에서 나온 모든 state값을 활용하지 않고 단순히 마지막에 나온 hidden vector를 하나의 고정된 context vector로 사용하였다. 이는 seq2seq model이 장기 기억 한계에 부딪힌 가장 큰 원인으로 볼 수 있다. 왜냐하면 context vector는 Encoder부분의 정보를 압축하고 있는 것인데 문장이 길어질 수록 정보 압축 시 정보 손실이 발생하기 때문이다. 이러한 문제를 해결하기 위해 고안된 Attention Mechanism에서는 Encoder에서 나온 각각의 state값을 모두 활용하여 Decoder부분에서 dynamic하게 context vector를 만들어 하나의 고정된 context vector의 사용으로 인해 발생한 seq2seq model의 문제를 해결하였다. 즉, 각각의 state 별로 context vector를 새롭게 만들어내 seq2seq model의 한계를 개선한 것이다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/29/vlm_architecture/',\n",
                            "     'source_title': 'VLM : Architecture / Fusion Methods\\n      29 May 2024',\n",
                            "     'text': '3.1 Contrastive Learning Contrastive Learning(대조 학습)은 입력된 image와 text를 동일한 vector space에 놓고 쌍을 이룬 image-text의 각 embedding 거리를 최소화 하면서 맞춰가는 방법이다.(맞지 않는 쌍 간의 거리는 최대화 시키다.) 이때 거리 최소화 작업에 사용되는 vector간 거리 산출 공식은 보통 cosine distance formula이다. OpenAI도 CLIP(Contrastive Language-Image Pre-training) 모델에서 cosine distance formula를 사용하였다. https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40 3.2 PrefixLM PrefixLM은 Transformer의 Encoder와 Decoder가 결합된 모델로, prefix token set이 주어졌을 때 다음 token을 예측하는 모델이다. PrefixLM은 visual embedding과 text embedding을 병렬적으로 처리한다. 아래 그림과 같이 Transformer의 Encoder가 concat된 image와 text emebedding(-> prefix emebedding)을 입력 받고 Decoder 그 뒤에 이어질 token을 예측한다. 예측 token은 text이다. SIMVLM: SIMPLE VISUAL LANGUAGE MODEL PRETRAINING WITH WEAK SUPERVISION 3.3 Multi-modal Fusing with Cross Attention Multi-modal Fusing with Cross Attention은 cross-attention mechanism을 통해 시각적 정보를 language model에 통합시키는 방법이다. 아래 이미지와 같이 visual encoder를 통해 image를 embedding한 후 이 embedding 결과를 language model의 cross-attention layer에 입력한다. 이 방법을 사용한 대표적인 모델은 VisualGPT이다. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/',\n",
                            "     'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
                            "     'text': '4. Evaluation Representation models 본 논문은 5.5B-word로 pre-trained된 ELMo의 weight를 사용한 ELMo model(1024 dimension)들과 BERTbase(768 dimension) 그리고 BERTlarge(1024 dimension)가 Penn Treebank의 parse tree를 얼마나 잘 재구성하는지를 통해 그들의 embedding 성능을 평가하고자 한다. BERT같은 경우에는 subword representation인데 이는 average pooling을해서 해결한다. 평가하고자 하는 representation은 ELMoK, BERTbaseK, BERTlargeK라고 부른다. Baselines 정확한 비교를 위해 linear, ELMo0, Decay0, PROJ0을 함께 살펴본다. -linear는 단순히 left-to-right chain을 따라서 parse tree가 형성되는 모델이다. 이 모델의 hidden state를 가져와서 proving한다. -ELMo0은 contextual 정보 없이 character-level에서 word embedding이 된 모델로, position정보도 없는 모델이다. -DECAY0은 weight가 단어의 수에 따라 깎이는 방식으로 학습되는 모델이다. 이 모델은 contextual 정보는 담지만 parsing하는 법은 학습하지 않은 모델이다. -PROJ0은 random initialized된 ELMo모델이다. 이 모델도 contextual 정보는 담지만 parsing하는 법은 학습하지 않은 모델이다. 4.1 Tree distance evaluation metrics'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/',\n",
                            "     'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
                            "     'text': '위 방법으로 Instruction-Following Dataset을 구축할 때 두 가지 문제가 있었다. 아래는 문제와 문제 완화를 위해 적용한 방법이다: 1. language-only GPT-4/ChatGPT를 사용했기 때문에 모델이 Visual Content를 인지할 수 없는 문제 이 문제 해결을 위한 대전제는 image를 LLM이 인식할 수 있는 시퀀스로 encoding 가능하게 하는 것이다. (i) Caption Caption을 GPT-4/ChatGPT에 함께 입력함으로써 language-only 모델이 visual scene을 알 수 있도록 한다. (ii) Bounding box visual scene에서 특정 객체의 위치 정보를 좌표로 prompt에 넣어주어 language-only 모델이 객체를 인식할 수 있도록 한다. 2. Instruction-following data 유형 정의 및 유형별 포함 내용 정의 문제 Conversation, Detailed description, Complex reasoning 유형을 구축하여 세부 사항을 정의하여 총 158,000개의 data를 구축하였다. (i) Conversation (58,000개 생성) - 주어진 이미지에 대해 Assistant와 Human이 대화하는 형태로 구성. - Assistant가 image를 보고 Human의 질문에 답하는 것과 같은 형식. - image 내의 직관적인 시각적 정보에 대한 QAset으로 생성. (객체 유형, 객체 수, 객체 동작, 객체 위치, 객체 간의 상대적 위치 등) (ii) Detailed description (23,000개 생성)'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/',\n",
                            "     'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
                            "     'text': \"['내', '가', '어제', '호떡', '을', '먹', '고', '싶', '어서', '사', '온', '건데', '나', '를', '빼', '고', '다', '먹', '었', '더라고'] kor_vocab = {} # 토큰화된 단어들을 {토큰:인덱스}형태로 저장할 딕셔너리를 만든다 kor_bow2 = [] # bow결과를 저장할 리스트를 만든다 for tok in kor_tokens: # 토큰화된 것을 하나씩 돌면서 if tok not in kor_vocab.keys(): # kor_vocab이라는 딕셔너리의 key값에 tok이 없으면 # 딕셔너리 key값으로 tok을 하나 넣고 그의 value값은 딕셔너리의 길이, 즉 원소 개수 kor_vocab[tok] = len(kor_vocab) # 그리고 리스트 kor_bow에는 '딕셔너리 길이 -1'인덱스 자리에 1을 넣는다 kor_bow2.insert(len(kor_vocab)-1, 1) else: # kor_vocab이라는 딕셔너리의 key값에 tok이 있으면 index1 = kor_vocab.get(tok) # 딕셔너리에서 key값으로 tok을 가지는 것의 value값을 index1에 넣는다. # 리스트에서 가져온 인덱스 값에 해당하는 것에 1을 더한다. kor_bow2[index1] = kor_bow2[index1] + 1 print(kor_bow2) print(kor_vocab)\"},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/',\n",
                            "     'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
                            "     'text': 'Vision Encoder에서 Last Layer Feature를 사용하는 경우, Before the laset Layser Feature를 사용하는 경우보다 0.96%낮은 성능을 보였다고 한다. 이에 대한 이유로 추측되는 것은 마지막 Layer는 그 직전 Layer보다 일반적이고 추상적인 이미지 속성을 담고 있기 때문으로 추정된다. Answer를 먼저 예측하는 경우. 12Epoch에서 89.77%에 도달하였지만, Reasoning을 먼저 예측하는 경우에는 단 6 Epoch만으로도 89.77%에 도달하였다. 최종으로는 89.96%. 그래서 저자들은 Reasoning을 먼저 하는 것이 더 좋지 않을까 했지만 CoT 전략이 학습 속도 개선에는 좋지만 최종 성능에는 큰 영향을 미치지 않았다고 한다. Pre-train을 거치지 않은 경우, Pre training을 했을 때보다 5.11% 성능이 낮았다. 이를 통해 Pre training의 중요성이 강조되었다. 기존 13B 모델을 7B fh skwcns rufrhk 1.08% 성능이 감소하였다. 이에 따라 모델 크기에 따른 성능 차이가 존재함이 증명되었다. Visual Instruction Tuning Reference Visual Instruction Tuning https://llava-vl.github.io/ × Search'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/03/04/similar_search/',\n",
                            "     'source_title': 'RAG : Similarity search (Keyword Search, Semantic Search, Hybrid Search)\\n      04 Mar 2024',\n",
                            "     'text': '최근 생성형 모델과 함께 사용되는 RAG의 retrieval 단계에서는 qeury와 유사한 chunk를 찾는 것이 매우 중요하다. 검색된 chunk가 모델에 참고 문서로 입력되기 때문에 유사도 검색 결과가 최종 결과에 큰 영향을 미친다. 이에 따라 RAG 최종 답변 품질 향상을 위해 LLM 성능 개선뿐만 아니라 유사도 검색 성능 개선도 중요하다. Keyword Search BM25는 TF-IDF 알고리즘을 기반으로 한 키워드 검색 알고리즘이다. 매우 오래된 알고리즘이고, 이를 기반으로 한 여러 variation들이 제안되었지만 keyword search에 있어서 아직까지는 클래식이 베스트이다. langchain에서 제공하는 keyword search 라이브러리도 BM25를 사용하고 langchain_community.retrievers.bm25, ElasticSearch 5.0 이상부터도 기본 유사도 알고리즘으로 BM25을 사용하고 있다. from rank_bm25 import BM25Okapi query = \"키워드 검색 방법에 대해 알려줘\"'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/',\n",
                            "     'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
                            "     'text': 'LM의 추가학습이 downstream task에 미치는 영향과 그 이유를 분석하기 위한 다양한 실험들이 진행된다. 우선 4 가지 domain(biomedical, computer science publications, news , reviews)을 기반으로 각 domain마다 두 개의 classification task, 총 8개의 task에 대한 실험을 진행했으며, low resource와 high resource setting에서의 실험도 진행했다. 위 4가지 domain을 선정한 이유는 각 domain마다 text classification data가 있어서 DAPT와 TAPT에 대한 성능 비교가 용이하기 때문이다. 또 다른 실험으로는 DAPT 및 TAPT 수행 시 타 domain이나 task에 대해서도 knowledge transfer 되는지에 대한 실험이 있다. 마지막으로 인간이 만든 dataset과 자동으로 선택된 unlabeled dataset으로 진행한 pretraining에 대한 실험도 진행했다. 2. Domain-Adaptive Pretraining Domain-Adaptive Pretraining실험에는 4개의 domain(biomedical, computer science publications, news , reviews)에 대해 각각 biomedical (BIOMED) papers, computer science (CS) papers, newstext from REALNEWS 그리고 AMAZON reviews data가 사용되었다. 2.1 Analyzing Domain Similarity DAPT를 수행하기 전, 앞으로 진행할 실험 결과에 대한 명확한 분석을 위해 각 domain data와 Roberta가 학습한 data(original LM data)의 유사도를 확인하였다. Figure 2는 각 domain vocab에서 stopword를 제외하고 가장 많이 등장한 10000개의 단어에 대한 유사도를 측정한 결과이다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/',\n",
                            "     'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
                            "     'text': '가장 간단하게 chat history를 다루는 방법이다. retrieval 결과+query와 함께 chat history를 LLM에 입력하는 방식이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/',\n",
                            "     'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
                            "     'text': '결론적으로 BERT가 영어의 구조적 특성을 잘 파악한다는 것을 증명하였다. 한국어나 독일어에 적용한 사례가 있는지는 모르겠다… 찾아보자… Reference Jawahar.G et al.”What does BERT learn about the structure of language?,” Association for Computational Linguistics.2019 Sang et al. “Introduction to the CoNLL-2000 Shared Task Chunking,” CONLL/WS. 2000 × Search'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/',\n",
                            "     'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
                            "     'text': 'F(w_i^T\\\\tilde{w}_ {k})=P_{ik} \\\\end{matrix} \\\\begin{matrix} exp(w_i^T\\\\tilde{w}_ {k})=P_{ik} \\\\end{matrix} \\\\begin{matrix} w_i^T\\\\tilde{w}_ {k}=\\\\log(P_ {ik})=\\\\log(X_ {ik})-\\\\log(X_i) \\\\end{matrix} 하지만 위에서 언급했 듯이 center word($w_i$ ,$w_j$)와 context word($\\\\tilde{w}_k$)간의 상호 교차는 자유롭게 이루어질 수 있어야 하는데 위 수식의 $log(P_{ik})$는 $log(P_{ki})$과 상호 교차될 수 없다. 즉 전자와 후자는 각각$log(X_{ik})-log(X_i)$와 $log(X_{ki}) - log(X_k)$인데 두 수식이 서로 다르기 때문에 상호 교차가 불가능하다. 이를 해결하기 위해 $log(X_i)$에 대해 $w_i$는 bias $b_i$로, $\\\\tilde{ w }_{ k }$는 bias $\\\\tilde{ b }_{ k }$로 처리한 후 각 변에 해당 상수항을 다시 더한다: \\\\begin{matrix} { w }_ { i }^{ T }\\\\tilde { { w }_ { k } } =\\\\log { X_ { ik }-{ b }_ { i }-\\\\tilde { { b }_ { k } } } \\\\end{matrix} \\\\begin{matrix} { w }_ { i }^{ T }\\\\tilde { { w }_ { k } } +{ b }_ { i }+\\\\tilde { { b }_ { k } } =\\\\log { X_ { ik }} \\\\end{matrix} 이제 $d$차원 벡터 공간에 우변과의 차이를 최소화한 좌변의 값이 embedding된다. 즉 $d$차원 벡터 공간 안에 동시 등장 확률이 높은 단어 vector간의 거리는 가깝게, 낮은 단어 vector간의 거리는 멀게 배치된다. 이를 수식으로 정의하면 다음과 같다: \\\\begin{matrix}'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/',\n",
                            "     'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
                            "     'text': '위 그림은 각 모델이 그린 parse tree이다. 윗부분의 검은 선은 실제 정답이고 아랫부분의 선은 structural prove가 예측한 것이다. BERTlarg16은 실제 정답과 거의 일치하는 것을 볼 수 있다. 본 연구에서 BERT를 학습시킬 때 parse tree를 직접 학습시키지 않았음에도 불구하고 주변 문맥을 고려하여 parse tree를 제대로 예측하였다. 이를 통해 embedding공간에 parse tree가 반영된 선형변환된 공간이 있다는 것을 확인할 수 있다. ELMo1의 경우에는 BERT보다는 부정확하다. edge들이 조금씩 다르다. PROJ0은 비교를 위해 사용된 baseline모델로, parsing능력이 없어 아주 엉망진창인 것을 확인할 수 있다. 위 시각화 자료에서 왼쪽이 gold parse distance matrix이고 오른쪽이 structural prove가 예측한 결과인데 매우 비슷하게 잘 나온 것을 확인할 수 있다. 색이 어두울수록 가까운 것이고 밝을수록 먼 것이다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/',\n",
                            "     'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
                            "     'text': 'DocLLM attention mechanism DocLLM은 text token과 그에 대한 bounding box 정보가 쌍을 이룬 data를 input으로 받는다. 본 모델은 bounding box를 separate hidden vector로 encoding하고 attention mechanism을 네 가지 score로 분해한다: text-to-text, text-to-spatial, spatial-to-text, spatial-to-spatial. 각각의 점수의 균형을 맞추기 위해 projection matrix와 hyperparameter를 사용한다. spatial information에 대한 hidden vector는 layer 전반에 걸쳐 재사용된다. DocLLM의 input은 $\\\\mathrm{{(x_i,b_i))}}_{i=1}^{T}$으로 표현되는데 여기에서 $b_i$은 $x_i$에 해당하는 bounding box (left, top, right, bottom)이다. LLM이 spatial information을 포착할 수 있도록 하기 위해 bounding box를 hidden vector $S\\\\in \\\\mathbb{R}^{T\\\\cdot d}$로 encoding한 후 attention matrix를 네 가지 다른 score로 분해한다: text-to-text, text-to-spatial, spatial-to-text, spatial-to-spatial.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/',\n",
                            "     'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
                            "     'text': 'T5에 대한 자세한 내용은 논문을 보는 게 좋고 간략한 내용은 Google Research Blog를 보는 것이 좋다. Closed-Book Question Answering demo applicaion도 체험할 수 있다 Bidirectinoal Language Model mask token에 들어갈 token을 앞뒤 token을 기반으로 예측하는 방식으로 학습하니까 양방향 LM 여기에 속하는 모델들은 transformer의 encoder 구조를 기반으로 하기 때문에 self-attention을 사용한다. self-attentiondms 해당 time step의 양 옆의 단어들을 모두 고려하여 attention을 계산하기 때문에 gpt계열보다 sequence의 맥락 정보를 더 잘 학습한다. BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding transformer의 encoder구조를 기반으로 만들어진 모델. mlm과 nsp task로 학습됨. mlm : input문장의 특정 token에 mask를 취하고 mask token에 위치할 실제 token을 예측하는 task nsp: 문장 두 개를 입력 받아 두 문장이 뒷 문장이 앞 문장과 이어지는 문장인지 아닌지 예측하는 task bert 계열의 모델들은 위와 같은 task를 통해 사전학습을 충분히 거친 이후 task specific한 head를 붙이는 방식으로 downstream task를 푼다.(e.g. classification task: classification head / qa: span prediction head) RoBERTa : A Robustly Optimized BERT Pretraining Approach Roberta는 BERT계열의 모델이다. BERT와 구조는 거의 같은데 BERT가 under training되었다고 주장하며 이를 해결하기 위한 네 가지 방법을 제안했다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/',\n",
                            "     'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
                            "     'text': 'NPLM의 구조를 보면 크게 input layer와 hidden layer 그리고 output layer가 있다. 설명을 위해 간단한 예를 들겠다. “퇴근 시간 서울역은 최악이다”라는 문장에서 [퇴근], [시간], [서울역은] 이렇게 세 개의 단어가 주어진 경우에 [최악이다]라는 단어를 예측한다고 가정하겠다. 이때 Input layer에서는 우선 주어진 세 개의 벡터에 대해 행렬$C$와 내적한다. 이후 내적하여 나온 값들을 concatenate하면 입력 벡터 $x$가 만들어진다. 입력 벡터 $x$에 대한 정의는 다음과 같다: \\\\begin{matrix} x=[{ { x }_ { t-1 } },{ { x }_ { t-2 } },… ,{ { x }_ { t-n+1 } }] \\\\end{matrix} 위 식에서 $n$은 n-gram의 개수이다. 즉 예측 대상 단어를 포함한 단어의 개수이고 $t$는 문장 내에서 해당 단어가 등장한 순서를 나타낸다. 이렇게 구한 $x$를 $H$와 내적한 뒤 bias term ’$d$’를 더한 후 tanh를 적용시켜 hidden layer를 만들고, 거기에 $U$를 내적한 후, bias term ’$b$’를 더해주면 $y_{w_t}$가 구해진다: \\\\begin{matrix} { y }_ { { w }_ { t } }=b+U\\\\cdot tanh(d+H_x) \\\\end{matrix}'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/',\n",
                            "     'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
                            "     'text': '[Pretraining] dataset: filtered CC-595K subset epoch: 1 learning rate: 2e-3 [Fine-tuning] dataset: LLaVA-Instruct-158K dataset epoch: 3 learning rate: 2e-5 batch size: 32 4.1 Multimodal Chatbot 4.1.2 Qualitative Evaluation (정성평가) Multimodal Chatbot 추론 능력 평가를 위해 chatbot demo를 만들고 아래 표와 같이 in-depth image understanding을 요구하는 예제들을(GPT-4 논문에 나온 예제들) LLaVA, GPT-4, BLIP-2, OpenFlamingo에 입력하여 비교했다. BLIP-2와 OpenFlamingo는 ser’s instructions에 따르기보다는 장면에 대해 간단하게 설명하는 반면 LLaVA는 user’s instructions에 따라 정확하게 답변하는 것으로 확인되었다. 그리고 GPT-4와 비교했을 때, 유사한 추론 결과를 보이면서 보다 포괄적인 답변을 반환하는 것으로 확인된다. LLaVA는 비교적 소규모인 80K unique image를 학습한 모델임에도 학습 이미지와 다른 도메인에 속하는 이미에 대해서도 잘 이해하고 사용자의 요구에 맞게 잘 답변하는 것으로 확인되었다. Visual Instruction Tuning'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/08/01/llama3_1/',\n",
                            "     'source_title': 'LLaMa 3.1\\n      01 Aug 2024',\n",
                            "     'text': 'https://ai.meta.com/blog/meta-llama-3-1/ https://llama.meta.com/ × Search'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/',\n",
                            "     'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
                            "     'text': 'Pre-training procedure - Training options 본 논문에서 학습 시 사용한 corpus와 설정한 hyperparameter들은 다음과 같다: Corpus : BooksCorpus(800M words) + English Wikipedia(2500M words) Tokens : 37000WordPiece tokens Train batch size : 256 sequences (256sequences *512tokens = 128000 tokens/batch) Steps :1M Epoch : 40epochs Adam learning rate : 1e-4 Weight decay : 0.01 Drop out probability : 0.1 Activation function : GELU - Environmental setup Bert base : 4Cloud TPU(16TPU chips total) Bert large : 16Cloud TPU(64 TPU chips total) ≈72 P100 GPU Training time : 4days Fine-tuning pre-trained 모델에 classification layer를 추가하여 fine-tuning을 통해 특정 task를 수행한다. 이렇게 학습된 BERT의 parameter는 똑같이 사용고 task에 맞는 output layer만 추가하면 다양한 task를 하나의 모델을 통해 수행할 수 있다.이것이 BERT가 대부분의 NLP task에 대해 SotA를 달성 할 수 있었던 핵심으로 생각된다. fine-tuning시에는 input으로 목적에 맞는 labeled dataset을 사용해 fine-tuning하여 최종 모델을 만든다. 해당 논문에서 실험한 fine-tuning task들은 아래 그림에서 확인할 수 있다:'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/',\n",
                            "     'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
                            "     'text': '본 논문의 저자인 Rui P. Chaves는 island effect를 전공한 사람이다. 그래서 experiment syntax 연구 모델을 활용하여 본 실험을 설계한 것이다. a, b, c, d에서 b와 c는 정문이고 a와 d는 비문이다. 이와 같은 2x2 design은 a-b, a-c, c-d 각각이 syntactic constraint를 기준으로 하나의 minimal pair가 된다. 이런 식으로 2x2 conditional design으로 정석적인 experiment syntactic method를 사용하였다. 20개의 item은 각 항목마다 lexicalization을 해서 20개씩 분포했다는 말이다. 이러한 metric은 human evaluation에서도 매우 좋은 설계이다. 3.2.1 Experiment 3 result: Filler-gap surprisal in subject-inverted interrogatives'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/',\n",
                            "     'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
                            "     'text': '\\\\[\\\\begin{align*} minimize J &=-\\\\log P(w_c|w_{c-m},...,w_{c+m})\\\\\\\\ &=-\\\\log P(u_c|v^)\\\\\\\\ &=-\\\\log \\\\frac{exp(u_c^{\\\\intercal}\\\\hat{v})}{\\\\sum^{|V|}_{j=1}exp(u_j^{\\\\intercal}\\\\hat{v})}\\\\\\\\ &=-u_c^{intercal}\\\\hat{v}+\\\\log\\\\sum^{|V|}_{j=1}exp(u_j^{\\\\intercal}\\\\hat{v}) \\\\end{align*}\\\\] $u_c$와 $v$를 최적화(optimization)시키는 방법으로 SGD(stochastic gradient descent)가 사용된다. 지금까지 학습 진행과정을 살펴보았다. 학습이 완료된 후에는 지금까지 학습시킨 파라미터 중 W를 embedded vector matrix로 사용한다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean.”Efficient Estimation of Word Representations in Vector Space,”(2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean.”Distributed Representations of Words and Phrases and their Compositionality,”(2013) × Search'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/',\n",
                            "     'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
                            "     'text': '104 두려움 무섭다 슬픔 비통하다 0.596 105 두려움 무섭다 미안함 죄송하다 0.596 106 미안함 미안하다 미안함 죄송하다 0.586 107 미안함 미안하다 감사 고맙다 0.518 108 미안함 미안하다 혐오 서운하다 0.582 109 미안함 미안하다 혐오 속상하다 0.633 110 미안함 죄송스럽다 미안함 죄송하다 0.440 111 미안함 죄송스럽다 혐오 속상하다 0.528 112 미안함 죄송스럽다 혐오 서운하다 0.563 113 미안함 죄송스럽다 혐오 괘씸하다 0.580 114 미안함 죄송스럽다 슬픔 슬프다 0.589 115 미안함 죄송하다 미안함 죄송스럽다 0.440 116 미안함 죄송하다 미안함 미안하다 0.586 117 미안함 죄송하다 두려움 무섭다 0.596 118 분노 화나다 혐오 속상하다 0.482 119 분노 화나다 슬픔 억울하다 0.544 120 분노 화나다 혐오 섭섭하다 0.593 121 분노 화나다 혐오 서운하다 0.608 122 상쾌함 개운하다 상쾌함 홀가분하다 0.513 123 상쾌함 개운하다 상쾌함 상쾌하다 0.556 124 상쾌함 개운하다 복합감정 시원섭섭하다 0.592'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/',\n",
                            "     'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
                            "     'text': 'Seq2seq모델은 말 그대로 sequence data가 들어가서 sequence data가 나오는 모델로, 간단히 말하자면 many-to-many방식의 RNN인데 encoder, decoder, generator를 통해 높은 확률의 결과 값을 도출하도록 학습하는 모델이다. 2.1 Architecture of seq2seq model'},\n",
                            "    {'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/',\n",
                            "     'source_title': '한국어 PDF Parser / PDF OCR (layout detection + text extraction)\\n      02 Feb 2024',\n",
                            "     'text': '1.2.2 HJDataset HJDataset은 거대 Aisan language dataset으로, 이를 학습한 모델로 문서 layout을 감지하는 task를 수행한다. text와 table 전반적으로 다 잘 감지하지 못함. 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.2.3 PubLayNet 결과가 포괄적인 경향이 있음. 제목 위치가 layout에 포함되지 않는 등의 문제가 있음. 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.4 PDF to XML PDF 구조를 XML로 변경하여 layout 파악(PDFQuery 사용) 1.4.1 PDF to XML 시각화 표가 그림으로 삽입된 페이지 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.4.2 PDF to xml + PyMuPDF PyMuPDF 일반 text는 대부분 잘 추출하는 편. 표가 그림으로 삽입된 페이지의 table data 추출 못함. 표가 문서 내 표로 삽입된 페이지의 table data의 경우 텍스트는 추출되지만 정리되지 않음. 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 2. Text Extraction 2.1 microsoft/Phi-3 image에서 한국어 인식 잘 못함.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
                            "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
                            "     'text': 'model의 전체 layer를 학습시키기 위해서는 많은 계산비용이 요구되는데 이는 곧 긴 훈련 시간과 큰 컴퓨터 자원 요구로 이어진다. Catastrophic forgetting문제 발생 가능성이 있다. 이는 모델이 모델이 새로운 task에 대해 학습하는 동안 pre-training 과정에서 배운 general knowledge를 잊어버리는 문제이다. 이는 잘 학습된 사전학습 모델을 무쓸모로 만든다. 모델의 전체 wight가 업데이트되기 때문에 사전학습 모델과 동일한 크기의 모델이 서버에 또 저장되기 때문에 용량 문제가 생길 수 있다. 위와 같은 문제를 완화하기 위해 Parameter-Efficient Fine-Tuning와 같은 대안들이 많이 연구되고 있다. 3.2 Parameter-Efficient Fine-Tuning (PEFT) Parameter-Efficient Finetuning (PEFT) methods 3.3 Representation Fine-tuning (ReFT) Pre-trained Model의 가중치는 frozen시키고, Model의 representation의 일부를 조작하여 downstream task를 해결하도록 하는 방법이다. ReFT의 종류 중 하나인 Low-rank Linear Subspace(LoReFT)는 PEFT보다 10배에서 50배 더 parmeter를 효율적으로 사용한다고 한다. 4. Fine-tuning variations : RLHF/PPO, DPO, ORPO Fine-tuning variations : RLHF/PPO, DPO, ORPO Reference'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/',\n",
                            "     'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
                            "     'text': '사전 학습 후에는 네 가지 핵심 문서에 대해 intelligence task에 대한 fine-tuning(instruction-tuning)을 수행한다. 본 연구의 contribution은 아래와 같다: visual document를 이해하기 위해 설계된 light-weight LLM 개발 text and layout modality 간의 cross-alignment를 위한 분리된 attention mechanism 제안 불규칙적인 layout을 효과적으로 파악하기 위한 infilling pre-training objective 도입 visual document intelligence task 수행을 위해 특수 생성된 instruction-tuning dataset 구축 2. Related Work 2.1 LLM Architectures Autoregressive Infilling autoregressive infilling approach에는 두 가지 방법이 있다. FIM(fill-in-the-middle): single span 예측 GLM(General language model pretraining with autoregressive blank infilling): multiple span 예측'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': 'Smith & Levy (2013)의 연구에 따르면 n-1에 기반하여 다음 단어를 예측하는 N-gram모델로도 reading behavior를 잘 모사했다고 한다. 2.2.2 Grammatical Language Model vs. RNN Language Model 사람은 Incremental Parsing과 같은 문법 기반의 언어모델을 통해 문장을 처리한다. 하지만 이런 Parser기반의 언어모델이 단순한 Sequential model보다 더 효율적이라는 것을 수학적으로 증명하기란 쉽지 않다. 증명이 어려워서 심지어 Frank & Bod(2011)에서는 기계가 문장을 이해하는데 있어 통사적 정보가 불필요하다는 주장이 나오기도 했다. 근데 이건 아닌 것 같다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/',\n",
                            "     'source_title': 'Contextual word representation\\n      30 Mar 2021',\n",
                            "     'text': 'https://nlp.stanford.edu/~johnhew/structural-probe.html#the-structural-probe × Search'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/',\n",
                            "     'source_title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
                            "     'text': 'MQA: 성능 저하와 학습이 불안정하다는 한계를 지니는데 이를 해결하기 위해 제안된 방법론이 GQA이다. GQA: H개 존재하던 K, V head를 G개의 그룹으로 줄이는 것이다. (즉, G=1일 경우 MQA가 되는 것이기 때문에 MQA와 MHA를 적절히 섞은 방법론이라고 볼 수 있다.) 아래 표는 동일한 모델에서 Attention 방식만 바꾸어 실험한 MHA, MQA, GQA 비교 결과이다. 3. Fine-Tuned Chat Model 대화 use-cases에 최적화된 fine-tuned version self-supervised learning으로 학습된 LLAMA2를 fine-tuning하여 chat version을 만듦 3.1 fine-tuning methodology Foundation Model LLAMA2를 SFT(Supervised Fine-Tuning)하여 LLAMA2-Chat을 생성 SFT를 통해 만들어진 LLAMA2-Chat에 RLHF(Reinforcement Learning from Human Feedback)를 수행하여 반복적으로 튜닝 1) Supervised Fine-Tuning(SFT) LLAMA2-Chat 개발진들은 처음에 publicly available instruction tuning data를 사용하였는데 데이터 품질 문제로 인해 직접 FT data를 수집했다. 27,540개의 high-quality SFT data를 수집하여 학습에 적용한 결과, 비교적 적은 양의 고품질 데이터가 다량의 저품질 데이터보다 모델 성능 향상에 기여함을 확인하였다. 수집한 SFT data는 아래 샘플과 같이 Prompt와 Response로 구성된다. 2) Reinforcement Learning with Human Feedback(RLHF) : Human Preference Data Collection'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/28/vlm_task/',\n",
                            "     'source_title': 'VLM : Applications\\n      28 May 2024',\n",
                            "     'text': 'image (혹은 video)와 해당 image에 대한 질문을 text로 입력받아 text로 질문에 대한 답을 반환하는 task. 예를 들어 아래 그림에 대해 “저 꽃은 무슨 색인가요?”라는 질문을 하고 “주황색”이라는 답변을 받는 task이다. Openai Image generator 1.2 Visual Captioning and Description Illustrated by the author image에 대한 설명을 생성하는 task. image를 입력 받아 text로 설명을 반환하거나, image와 prompt를 함께 입력 받아 설명을 반환할 수도 있다. 1.3 Identifying Objects in Images with Textual Cues Illustrated by the author image와 그에 대한 설명을 입력 받아 설명에 등장한 객체를 이미지 내에서 찾아내는 task. 예를 들어 아래 그림과 함께 “사람이 물을 마시고 있다” 라는 caption을 입력받으면 입력된 텍스트 정보 내에 등장한 객체(“사람” 과 “물”)을 이미지 내에서 찾는 task이다. Openai Image generator 1.4 Visual Commonsense Reasoning image를 입력 받아 image 내의 객체를 식별한 후 객체 간의 관계를 파악하여 이미지에 나타난 객체들의 상관관계를 상세히 설명하는 task. https://visualcommonsense.com/'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/',\n",
                            "     'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
                            "     'text': 'DocLLM은 16개 dataset과 그에 해당하는 OCR을 사용하여 4가지 DocAI task를 처리할 수 있도록 instruction-tuning된다. 4가지 DocAI task는 아래 표에 기재된 바와 같이 Visual Question Answering (VQA), Natural Language Inference (NLI), Key Information Extraction (KIE), Document Classification (CLS)이다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/',\n",
                            "     'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
                            "     'text': '3. Multimodel Transformers 최근 Transformer를 활용한 Multimodal 연구가 활발히 이루어지고 있다. 연구 결과, discriminative task와 generative tasks에서 모두 여러 modlaity에 대해 호환되는 것으로 확인되었다. 3.1 Multimodal Input 3.1.1 Tokenization and Embedding Processing 1) input tokenizing : 입력된 input의 modality에맞게 tokenizing한다. 2) representation embedding space 선택: tokenizing된 것들으 token의 modality에 따라 그에 맞는 representation embedding space를 선택한다. 예를 들어 아래 표와 같이 token의 modality가 RGB이면 token의 단위는 patch, embedding은 linear projection. token의 modality가 text이면 token 단위는 word, embedding은 learned embedding(Model parameter를 통해 embedding한 결과) 아래 표와 같이 다양한 modality에 대해 다양한 tokenizing 방식과 embedding space가 존재한다. Multimodal Learning with Transformers:A Survey 3.1.1 Token Embedding Fusion 각 modality별로 tokenizing-embedding을 모두 마친 이후 중요한 것 각 modality별 embedding을 어떻게 합쳐서 multimodal 처리를 가능하게 할 것인가이다. 가장 단순한 fusion은 여러 modality들에 대한 embedding들을 token-wise sum하는 것이다. transformer는 하나의 position에 대해 여러 token embedding을 반영할 수 있기 때문에 가능한 방법이다. 예를 들어 BERT의 경우 한 position embedding에 token embedding과 segment embedding을 position을 기준으로 원소값별로 더하여(element-wise sum하여) 모델에 입력한다. 이를 시각적으로 표현하면 아래 이미지와 같다. Multimodal Learning with Transformers:A Survey(edited by author)'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/',\n",
                            "     'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
                            "     'text': 'Model merging 기법은 여러 모델을 하나의 모델로 결합하는 기술이다. 최근 open llm leaderboard에도 많은 merge model들이 올라오고 있다. Mergekit 혹은 LazyMergekit을 통해 간편히 Model merge를 할 수 있다. 1. Task Vector Arithmetic “task vectors”라는 것을 통해 신경망의 동작을 변형/수정하는 방법이다. task vectors는 pre-trained model의 weight space에서 특정 task의 성능 향상을 가리키는 방향이다. 이 기법에서는 부정(negation) 혹은 덧셈과 같은 연산을 통해 vector의 수정/변형이 이루어진다. (model의 targeted behavior에 맞게) modifying “task vectors” Forgeting via negation : task vector에 대한 부정을 통해 target task의 성능은 줄이되 control task에 대해서는 성능을 유지할 수 있다. Learning via addition : task vector에 대한 덧셈을 통해 여려 task들에 대한 성능을 향상시킬 수 있다. Task analogiy : 유사한 task의 vector의 결합을 통해 추가 task에 대한 데이터 학습 없이도 해당 task 성능을 향상시킬 수 있다. 장점 Efficient Model Editing : 간단하고 효율적으로 효과적인 결합을 할 수 있다. Versatility Across Models and Tasks : 다양한 model과 task에 대해 잘 작동하는 것이 많은 사례를 통해 입증되었다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/',\n",
                            "     'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
                            "     'text': '1. Introduction 본 논문은 특정 task 혹은 domain에 대해 맞춤 학습을 수행한 LM(Language Model)이 더 좋은 성능을 보일 것이라는 전제를 기반으로 하며, 이를 다양한 실험을 통해 입증하였다. 최근 자연어처리 분야에서는 LM을 기반으로 downstream task에 대해 fine-tuning을 진행하는 방식이 많이 사용된다. 즉, LM에서 학습된 representation을 기반으로 해당 모델 뒤에 특정 task를 수행할 layer를 추가하여 학습하는 방식이 많이 사용된다. 수행하고자 하는 task가 LM을 기반으로 하는 만큼 fine-tuning model의 성능은 LM의 영향을 받게 된다. 이는 곧 LM의 data distribution이 downstream task 결과에도 영향을 준다는 의미로 이어질 수 있다. 오늘날 LM은 일반적으로 백과사전, 뉴스 기사 등 웹 크롤링 데이터를 학습한다. 1.1 Data distribution Figure 1은 data distribution을 그림으로 표현한 것이다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/',\n",
                            "     'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
                            "     'text': 'Same Datasets, Different Splits (SDDS) 일반적인 방법이다. 16개의 데이터셋에 대해 trainset을 학습하고, 각 trainset을 학습한 모델을 상응하는 testset으로 평가하는 방식이다. 이와 같은 방식 아래에서 zero-shot setting으로 실험한 결과, GPT-4와 Llama2를 포함한 다른 모델들보다 16개 데이터셋 중 12개에서 더 좋은 성능을 보였다. 특히 KIE와 CLS와 같이 레이아웃 집약적인 task에서 특히 좋은 성능을 보였다. 아래는 SDDS 실험 결과이다. DocLLM: A layout-aware generative language model for multimodal document understanding'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/',\n",
                            "     'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
                            "     'text': 'process를 memory에 올릴 때 process를 page 단위로 나누어 처리함. DoRA Low-Rank Adaption (LoRA)'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/',\n",
                            "     'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
                            "     'text': '위 그래프는 이 실험에 대한 결과이다. 본 실험은 앞에 있는 NP와 동사의 단복수가 다른 조건에서 surprisal이 낮아야 하는 상황이기 때문에 다른 실험들의 결과와는 반대로 나타나야 한다. 위 그래프를 보면 BERT가 문장을 잘 파악해서 주-동 수일치를 잘 식별한 것을 알 수 있다. 4.4 Experiment 3, 4 result: Filler-gap surprisal in subject-inverted interrogatives, Filler-gap surprisal in uninverted indirect interrogatives 이 실험은 앞선 실험과는 달리 long distance dependency를 도치와 비도치 의문문 조건에서 살펴본다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/',\n",
                            "     'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
                            "     'text': '이 방법론에서 중요한 것은 두 검색 결과를 적절히 결합하는 것이다. 검색 결과 결합에는 일반적으로 Reciprocal Rank Fusion algorithm이 사용된다. 4. Self-RAG 일반적인 RAG은 입력된 모든 query에 대해 vector DB에 저장된 전체 문서와 유사도를 구하여 관련 chunk를 찾아 답을 생성하는 방식으로 작동된다. Self-RAG의 경우, 아래 그림과 같이 익숙한 주제에 대해서는 LLM이 바로 답변하도록 하고, 그렇지 않은 주제에 대해서는 RAG을 수행하는 방식으로 작동된다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/',\n",
                            "     'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
                            "     'text': 'DocLLM: A layout-aware generative language model for multimodal document understanding 본 연구에서는 DocAI dataset의 탬플릿으로부터 파생된 다양한 instruction으로 instruction tuning을 수행했다. 또한 supervised fine tuning (SFT) 수행 시 입력되는 instruction의 다양성이 높을수록 해당 모델의 zero-shot generalization 능력이 향상되기 때문에 최대한 다양한 template을 사용하려고 했다(“Finetuned language models are zero-shot learners”, “Scaling instruction-finetuned language models”, “ Training language models to follow instructions with human feedback”). 각 template은 다른 질문을 제시하고, 몇몇 경우 다른 유형의 답변을 반환하도록 했다. dataset의 탬플릿은 “Modularized multimodal large language model for document understanding”과 “Universal ocr-free visually-situated language understanding with multimodal large language model” 연구에서도입된 탬플릿을 가능한 재사용하였다. Visual Question Answering(VQA) DocVQA, WikiTableQuestions(WTQ), VisualMRC, DUDE, BizDocs2를 수집하여 VQA instruction-tuning data mix를 구축한다. VQA 학습 시 input은 위 표에 나타난 바와 같이 하나의 instruction template이다. {document} {question} e.g. {document} What is the deadline for scientific abstract submission for ACOG - 51st annual clinical meeting? Natural Language Inference(NLI) 본 연구에서 구축한 instruction-tuning data mix를 DocAI NLI dataset으로 사용하기에는 수량 부족한 관계로, 여기에 TabFact dataset을 추가하여 사용한다. {document}\\\\\"{statement}\\\\\", Yes or Ne? e.g. {document} \\\\\"The UN commission on Korea include 2 Australians.\\\\\", Yes or No?'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/',\n",
                            "     'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
                            "     'text': 'ELECTRA모델은 generator와 discriminator로 구성되어 있다. discriminator에서는 token이 generator에서 다른 token으로 대체되었는지를 예측한다. generator와 discriminator의 크기가 동일하다면 weight sharing이 가능하다. Hybrid Language Model XLNet × Search'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/',\n",
                            "     'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
                            "     'text': \"BOS(Bag of Words) # 벡터 개수 세는 라이브러리를 불러온다. from sklearn.feature_extraction.text import CountVectorizer bos_sentence = ['betty bought a bit of better butter to make her bitter butter better'] count_words1 = CountVectorizer() # vector 세어줄 라이브러리를 담은 객체를 생성한다 bow1 = count_words1.fit_transform(bos_sentence) # 문장에 fit_transform해준다 print(bow1.toarray()) # 벡터 센 결과를 리스트 형태로 출력한다. print(count_words1.vocabulary_) # 결과에서 어떤 단어에 대한 카운트가 어느 인덱스에 있는지 출력한다. [[2 1 1 1 1 2 1 1 1 1]] {'betty': 1, 'bought': 4, 'bit': 2, 'of': 8, 'better': 0, 'butter': 5, 'to': 9, 'make': 7, 'her': 6, 'bitter': 3} # 불용어는 제거하고 카운트를 셀 수 있다. count_words2 = CountVectorizer(stop_words='english') bow2 = count_words2.fit_transform(bos_sentence) print(bow2.toarray()) print(count_words2.vocabulary_) # of, to, her이 불용어로 처리되어 제거됐다. [[2 1 1 1 1 2 1]] {'betty': 1, 'bought': 4, 'bit': 2, 'better': 0, 'butter': 5, 'make': 6, 'bitter': 3} # 보기 편하게 DataFrame으로 바꾸자 import pandas as pd from pandas import DataFrame bos_index = count_words2.vocabulary_ bos_result = bow2.toarray() bos_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.)\"},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/',\n",
                            "     'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
                            "     'text': 'Full Fine-Tuning에는 데이터, 컴퓨터 자원, 시간 등 대규모 자원이 요구되는 문제가 있다. Large Language Model이 많이 나오며 이렇게 자원이 많이 요구되는 Full Fine-Tuning을 대체하며 LLM을 효율적으로 학습시킬 수 있는 방법론들이 활발이 연구되고 있다. 해당 연구는 주로 LLM tuning 이후의 성능을 더 좋게 하거나 모델을 보다 빠르게 훈련시킬 수 있도록 하는 방향으로 진행되고 있다. 이와 같은 연구 중 하나인 Parameter-Efficient Fine-Tuning (PEFT)에는 다양한 변gud들이 존재하는데 대부분 상대적으로 적은 dataset으로 model의 일부 parameter를 업데이트하거나 소수의 새로운 파라미터를 추가하는 방법들이다. PEFT의 장점은 크게 세 가지로 볼 수 있다: Reduced memory footprint PEFT은 model의 파라미터 중 제한된 일부 parameter만 업데이트하고 model의 원래 parameter는 유지시켜 학습 시 요구되는 메모리를 크게 줄일 수 있다. Faster training 훈련 할 parameter, 즉 업데이트되는 parameter의 수가 적기 때문에 자연스럽게 훈련 시간도 단축된다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/',\n",
                            "     'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
                            "     'text': '역슬래시(\\\\)를 이용한 정규 표현식 문자 규칙 문자 설명 \\\\\\\\ 역슬래시 그자체 \\\\d digit. 모든 숫자(= [0-9]) \\\\D 숫자를 제외한 모든 문자(= [^0-9]) \\\\s space. 공백(= [ \\\\t\\\\n\\\\r\\\\f\\\\v]) \\\\S 공백을 제외한 모든 문자(= [^ \\\\t\\\\n\\\\r\\\\f\\\\v]) \\\\w word. 문자와 숫자(= [a-zA-Z0-9]) \\\\W 문자와 숫자를 제외한 다른 문자(= [^a-zA-Z0-9]) 정규표현식 | match | 영어 import re # 정규표현식ㅇ르 사용할 때 re 라이브러리를 불러와야 한다. check1 = \\'ab.\\' # ab. -> ab하고 \\'.\\'부분에 문자가 하나라도 오냐 print(\"about \\'abc\\': \", re.match(check1, \\'abc\\')) print(\"about \\'abc\\': \", re.match(check1, \\'abcd\\')) print(\"about \\'c\\': \", re.match(check1, \\'c\\')) print(\"about \\'ab\\': \", re.match(check1, \\'ab\\')) about \\'abc\\': <re.Match object; span=(0, 3), match=\\'abc\\'> about \\'abc\\': <re.Match object; span=(0, 3), match=\\'abc\\'> about \\'c\\': None about \\'ab\\': None 정규표현식 | match | 한국어 import re check2 = \\'[ㄱ-ㅎ]+\\' # ㄱ부터 ㅎ까지의 한글 자음이 나오고 그 뒤로 문자가 최소 하나 이상 오냐 # check2에 등록한 정규표현식에 걸리는 애를 찾아라 print(re.match(check2, \\'ㅎ안녕\\')) print(re.match(check2, \\'안녕ㅎ\\')) # ㄱ-ㅎ에 걸리는 \\'ㅎ 안녕\\'에서 ㅎ이 걸렸다. # 걸린게 없다 <re.Match object; span=(0, 1), match=\\'ㅎ\\'> None'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/',\n",
                            "     'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
                            "     'text': 'BERT는 2018년 10월에 SQuAD에서 기존의 앙상블 모델들의 기록들을 앞지르며 등장한 언어 모델로, Human Performance를 능가하여 큰 관심을 받은 모델이다. 그리고 pre-trained된 BERT모델에 classification layer만 추가하면 다양한 자연어처리 과제를 수행할 수 있어 데이터셋 환경이 좋은 영어권에서는 11개의 NLP task에 대해 SotA(State of the Art)를 달성하였다. BERT는 Bi-directional Encoder Representations form Transformers의 약자로, 이름 그대로 이전 게시물에서 소개한 Transformer모델의 encoder부분만 사용한 모델이다. Transformer에 대한 자세한 설명은 이전 게시물에 있기 때문에 해당 게시물에서는 Transformer설명과 중첩되는 부분은 생략하고 지나가겠다. BERT의 학습은 pre-training step과 pre-trained model을 가지고 fine-tuning을 하는 step으로 나뉜다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/',\n",
                            "     'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
                            "     'text': '(1) Linear: 원래 이전에 썼던 모델 (2) MLP: linear probing model을 MLP(multilayer perceptron; here used: single 1024d hidden layer activated by ReLU)으로 교체하여 더 많은 parameter를 probing model에 추가하는 실험. (그냥 다른 task처리는 전혀 안 하고 차원을 늘려 계산을 좀 더 복잡하게 하겠다는 것) (3) LSTM+ Linear: linear output layer이전에 task-trained LSTM(unidirectional, 200 hidden units)을 사용한 contextual probing model을 사용하여 task-specific한 contextualization을 추가하는 실험. (LSTM을 task에 맞게 훈련시키는 것. 주입식교육시켜서 feature를 더 잘 추출할 수 있게 학습시킨 모델을 probing model을 사용) (4) BiLSTM+MLP: CWR의 output이 BiLSTM(512 hidden unit)에 들어가고 이 output은 다시 MLP(single 1024-dimensional hidden layer activated by ReLU)에 들어가는 것.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/',\n",
                            "     'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
                            "     'text': 'Classification task : input으로 분류하고자 하는 문장, output으로는 분류 label. (label은 model vocab내에 있는 토큰 중 하나로 추론되기 때문에 label 목록 외의 것이 나올 경우에는 틀린 것으로 간주) Regression task : STS-B(semantic textual similarity : 텍스트 의미적 유사도 예측 과제)와 같은 regression task의 경우 특정 단위로 나누어서 그걸 라벨로 취급하여 classification task처럼 처리 (e.g. 1-5사이 스코어 추론 과제: 0.2 단위로 1, 1.2, 1.4, 1.6, …으로 나눔)'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/',\n",
                            "     'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
                            "     'text': '주요 구성요소 1) Agent: 학습을 수행하는 주체. 주어진 환경 내에서 행동을 선택하고 그 결과로 보상을 받는 주체. 2) Environment: Agent가 상호작용하는 세계. Agent의 행동에 대한 보상(feedback)을 제공한다. 3) State: Environment의 상태 4) Action(Action Space): Agent가 취할 수 있는 선택지 5) Reward: Agent가 특정 행동을 취했을 때 환경으로부터 받는 feedback. 이것을 기준으로 학습이 진행된다. 6) Policy: Agent가 특정 State에서 어떤 행동을 취할지 결정하는 전략 7) Value Function: 특정 State에서 시작하여 장기적으로 얻을 것으로 기대되는 보상들의 누적 총합. 8) Q-Function: 특정 State에서 특정 행동을 취했을 때 장기적으로 얻을 것으로 기대되는 보상들의 누적 총합. 학습과정(아래 과정을 반복하며 최적의 Policy를 얻는다.) 1) 초기화: Agent를 초기화 시킨 후 Agent의 초기 Policy를 가지고 Environment와 상호작용한다. 2) Action 선택: 현재 State를 기반으로 Policy에 따라 Action을 선택한다. 3) Environment와의 상호작용: Agent가 선택한 행동을 실행한 후 그 결과로 새로운 State와 Reward를 받는다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': '딥러닝에 사용할 만큼의 규모로 구축되지 않았다는 문제가 있다. 그렇다면 이제 또 다른 의문이 제기될 수 있다. 모델이 입력되는 대규모 코퍼스를 바로바로 parsing하면 되지 않느냐 물을 수 있다. 이러한 방법은 통사론적으로 매우 간단한 문장에서는 제대로 작동할 수 있지만 통사론적으로 조금만 복잡한 문장의 경우에는 parsing이 제대로 이루어지지 않는다. 하지만 우리는 통사론적으로 복잡한 문장을 딥러닝이 잘 분석하고 이해하기를 원하기 때문에 이와 같은 방식으로 도출된 결과는 딥러닝 모델에 사용할 수 없다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/',\n",
                            "     'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
                            "     'text': 'entity-based model은 각 mention을 이전 mention이 아닌 이전 담화 개체(mentions들의 cluster)에 연결한다. - entity-ranking model entity-ranking model은 단순히 mention-ranking model에서 classifier가 개별 mention들이 아니라 mention들의 cluster들에 대해 결정을 내리게 한 것이다. entity-based model도 mention-ranking model처럼 feature-based model과 neural model 둘 다를 통해 구현될 수 있는데, 이전의 모델들보다 표현력이 더 뛰어나지만 cluster-level 정보를 이용하는 것이 실제로 큰 성능 향상으로 이어지지 않았기 때문에 mention-ranking model이 여전히 더 많이 사용된다. 그래서 entity-based model은 간단하게만 소개하고 넘어가도록 하겠다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/',\n",
                            "     'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
                            "     'text': '놀람 고뇌 1 놀람 2 슬픔 1 혐오 1 동정 고뇌 2 고독 2 동정 2 슬픔 10 허무 1 혐오 2 두려움 고뇌 5 고독 1 두려움 2 미안함 1 슬픔 3 미안함 감사 1 두려움 1 미안함 4 슬픔 1 혐오 5 복합감정 상쾌함 1 분노 슬픔 1 혐오 3 상쾌함 고뇌 1 복합감정 1 상쾌함 5 슬픔 1 허무 1 수용 기쁨 9 수치심 수치심 5 슬픔 3 허무 2 혐오 4 슬픔 고뇌 4 고독 9 동정 5 분노 1 상쾌함 2 수치심 4 슬픔 21 허무 1 혐오 10 욕구 혐오 2 허무 고뇌 1 동정 1 수치심 2 슬픔 1 허무 2 혐오 1 혐오 고뇌 4 고독 1 놀람 1 동정 2 미안함 3 분노 2 수치심 4 슬픔 11 허무 1 혐오 13 후회 1 후회 혐오 1'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/',\n",
                            "     'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
                            "     'text': \"iter2: <callable_iterator object at 0x0000021C92438688> <re.Match object; span=(10, 11), match='$'> <re.Match object; span=(11, 12), match='$'> <re.Match object; span=(12, 13), match='^'> <re.Match object; span=(13, 14), match='%'> <re.Match object; span=(14, 15), match='&'> <re.Match object; span=(15, 16), match='#'> from konlpy.tag import Mecab × Search\"},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': '문장은 선형적이며 계층적이다. 문장 내에 존재하는 순서는 문장의 의미를 구성하는데 있어 매우 중요한 역할을 한다. RNN(Recurrent Neural Network)모델은 노드들 사이에 hidden layer를 삽입하여 hidden state의 결과가 계속 순환하며 문장 내 순서를 고려할 수 있게 만들어진 모델이다. 예를 들어 The cat is on the mat이라는 문장에서 cat 다음에 is가 올 확률이 높다는 것과 같은 방식으로 구조적으로 수용 가능한 문장을 형성할 수 있다. 하지만 통사적으로 간단한 문장이 높은 확률을 할당 받는다 하더라도 언어학자들이 연구하는 논란의 여지가 많은 문장들을 연구할 수 있는 수준이 되는 것은 아니다. 그래도 최근 언어모델(Linzen(2019)기준)은 언어학적 지식을 기반으로 통사적 능력(ability)를 측정하고 향상시키는데 높은 성능을 보이고 있다. 이렇게 언어학은 딥러닝 성능 향상의 기준점이 될 수 있다. 예를 들어 본 논문의 저자인 Tal Linzen은 2016년에 정문(grammatical)과 비문(ungrammatical)을 이용해서 언어모델이 정문에 더 높은 확률을 줄 수 있는지에 대한 실험을 하였는데 이에 대해'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/',\n",
                            "     'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
                            "     'text': '완벽히 이해할 수 있도록 하는 것이다.'},\n",
                            "    {'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/',\n",
                            "     'source_title': '한국어 PDF Parser / PDF OCR (layout detection + text extraction)\\n      02 Feb 2024',\n",
                            "     'text': 'pypdf.PdfReader로 text 추출 추출된 게 거의 없고, pdf 대부분이 imgage로 이루어져 있으면 ocrmypdf로 ocr한 이후 다시 pypdf.PdfReader로 text 추출 from pypdf import PdfReader import ocrmypdf def extract_text_from_pdf(reader): full_text = \"\" for idx, page in enumerate(reader.pages): text = page.extract_text() if len(text) > 0: full_text += f\"---- Page {idx} ----\\\\n\" + page.extract_text() + \"\\\\n\\\\n\" return full_text.strip() def convert(pdf_file): reader = PdfReader(pdf_file) # Extract metadata metadata = { \"author\": reader.metadata.author, \"creator\": reader.metadata.creator, \"producer\": reader.metadata.producer, \"subject\": reader.metadata.subject, \"title\": reader.metadata.title, } # Extract text full_text = extract_text_from_pdf(reader) # Check if there are any images image_count = 0 for page in reader.pages: image_count += len(page.images) # If there are images and not much content, perform OCR on the document if image_count > 0 and len(full_text) < 1000: out_pdf_file = pdf_file.replace(\".pdf\", \"_ocr.pdf\") ocrmypdf.ocr(pdf_file, out_pdf_file, force_ocr=True) # Re-extract text reader = PdfReader(pdf_file) full_text = extract_text_from_pdf(reader) return full_text, metadata convert(\"/workspace/PDF_Parsing/(20240603) 금융시장 브리프.pdf\") 3. Table Parsing 3.1 camelot-py 표가 그림으로 삽입된 페이지 테이블 감지는 잘 됨 테이블 내 text 추출이 잘 되지 않음 아래 그림을 보면 표 위치에서 감지하는 text가 없음 camelot-py table detection result(Illustrated by the author) camelot-py table text extraction'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': '이번에는 심리언어학적 실험통제가 딥러닝에 어떠한 도움을 줄 수 있는지를 다뤄보겠다. 딥러닝은 훈련을 하고 훈련 결과를 테스트한 후에 테스트 결과를 다시 훈련에 반영하는 과정을 반복한다. 하지만 이러한 훈련과 테스트 과정에는 동일한 데이터셋이 사용된다. 즉, 코퍼스 하나를 (일반적으로)7:3으로 나눠서 7은 traning set으로, 3은 test set으로 사용하는데 여기에서 문제가 나타난다. 우리가 코퍼스를 딥러닝 모델에 열심히 돌리는 이유는 뭔가 살펴보고자 하는 property가 있기 때문인데, 코퍼스에는 이와 관련된 정보가 따로 주석되지 않았기에 문제를 해결하기 어려운 것이다. 예를 들어 yes/no question 형식의 문장을 생성하고자 할 때, 코퍼스에는 문장 형식(의문문, 평서문, 명령문 등)에 대한 정보가 없기 때문에 모델이 yes/no question 형식을 찾아내는 것이 매우 어렵다. 그렇다면 이미 parsing이 된 Penn Treebank와 같은 코퍼스를 사용하면 되지 않냐는 의문이 제기될 수 있지만, 그러한 코퍼스들은 아직'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/',\n",
                            "     'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
                            "     'text': 'Key Information Extraction(KIE) Kleister Charity(KLC), CORD, FUNSD, DeepForm, PWC, SROIE, VRDU ad-buy (with random train-test splitting), and BizDocs를 수집하여 KIE instruction-tuning data를 구축하였다. instruction template 유형은 위 표에 기재된 바와 같이 총 3가지를 사용하였다: extraction, internal classification, MCQ. [Extraction] -> {document} What is the value for the \\\\\"{key}\\\\\"? e.g. {document} What is the value for the \\\\\"charity number\\\\\"? [Internal classification] -> {document} What is \\\\\"{value}\\\\\" in the document? [MCQ] -> {document} What is \\\\\"{value}\\\\\" in the document? Possible choices: {choices}. Document Classification(CLS) RVL-CDIP, BizDocs를 수집하여 CLS instruction-tuning data를 구축하였다. 이 작업에는 두 가지 유형의 template이 사용되었다. [MCQ] -> {document} What type of document is this? Possible choices: {choices}. e.g. {document} What type of document is this? Possible answers: [budget, form, file folder, questionnaire]. [Internal classification] -> {document} What type of document is this? 4. Experiments 4.1 Model Setup and Training Details 실험에 사용된 모델은 Falcon-1B를 기반으로 한 DocLLM-1B, Llama2-7B를 기반으로 한 DocLLM-7B로 두 가지 버전이 있다. 실험 시 사용된 hardware specification은 24GB A10g GPU 하나고 fully sharded data parallelism으로 학습이 진행되었다. 실험에 사용된 모델의 각종 config와 hyperparameter는 아래 표와 같다: DocLLM: A layout-aware generative language model for multimodal document understanding DocLLM-1B의 경우에는 1 epoch의 pre-training, 10 epoch의 instruct-tuning을 수행했고, DocLLM-7B의 경우에는 1 epoch의 pre-training, 3 epoch의 instruct-tuning을 수행했다. 4.2 Downstream Evaluation 본 연구에서는 두 가지 설정 하에 실험을 진행한다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/',\n",
                            "     'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
                            "     'text': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks randomly initialised soft token을 input prompt에 추가하고, LLM의 가중치는 frozen시킨 상태에서 해당 embedding을 학습시킨다. prompt를 추가함으로써 모델에는 형태가 입력된다. prompt에는 \"다음 문장에 대한 감성을 긍정, 부정, 중립으로 표시하시오\" 와 같이 모델이 task를 잘 인지하도록 하는 문구가 들어간다. 이 과정을 통해 frozen 된 모델이 해당 task에 대해 가장 답변을 잘하도록 만드는 prompt를 얻게 된다.'},\n",
                            "    {'source': 'https://finddme.github.io/dev%20log/2024/07/01/self_rag/',\n",
                            "     'source_title': 'Self-RAG + CRAG : Query Router | Reranker | Relevant Grader | Hallucination Grader | Answer Grader\\n      01 Jul 2024',\n",
                            "     'text': '# Prompt system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\\\n Give a binary score \\'yes\\' or \\'no\\'. Yes\\' means that the answer resolves the question.\"\"\" answer_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system), (\"human\", \"User question: \\\\n\\\\n {question} \\\\n\\\\n LLM generation: {generation}\"), ] ) answer_grader = answer_prompt | structured_llm_grader_answer Hallucination + Answer grader pipeline def grade_generation_v_documents_and_question(state): \"\"\" Determines whether the generation is grounded in the document and answers question Args: state (dict): The current graph state Returns: str: Decision for next node to call \"\"\" print(\"---CHECK HALLUCINATIONS---\") question = state[\"question\"] documents = state[\"documents\"] generation = state[\"generation\"] score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation}) # output : GradeHallucinations(binary_score=\\'yes\\') grade = score.binary_score # Check hallucination if grade == \"yes\": print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\") # Check question-answering print(\"---GRADE GENERATION vs QUESTION---\") score = answer_grader.invoke({\"question\": question,\"generation\": generation}) # output : GradeAnswer(binary_score=\\'yes\\') grade = score.binary_score if grade == \"yes\": print(\"---DECISION: GENERATION ADDRESSES QUESTION---\") return \"useful\" else: print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\") return \"not useful\" else: pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\") return \"not supported\" Set RAG Graph : langGraph from langgraph.graph import END, StateGraph workflow = StateGraph(GraphState) # Define the nodes workflow.add_node(\"websearch\", web_search) # web search # key: action to do workflow.add_node(\"retrieve\", retrieve) # retrieve workflow.add_node(\"grade_documents\", grade_documents) # grade documents workflow.add_node(\"generate\", generate) # generatae workflow.add_edge(\"websearch\", \"generate\") #start -> end of node workflow.add_edge(\"retrieve\", \"grade_documents\")'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/',\n",
                            "     'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
                            "     'text': '2. Sequence to Sequence Model 이제 본격적으로 Seq2Seq이라는 모델에 대해 소개하도록 하겠다. 일반적인 Deep Neural Network같은 경우 다양한 분야에서 좋은 성과를 보였으나 sequence data처리에는 큰 성과를 보이지 못 했다. 이는 input dimension과 output dimension이 고정되어 있기 때문이다. 물론 convolution연산의 경우 input의 크기가 정해져 있지 않지만 Multi-Layer-Perceptron의 경우에는 입력의 차원과 출력의 차원을 정한 후에 학습을 시작한다. 이러한 방식은 이미지 데이터와 같이 크기가 고정된 데이터를 처리할 경우에 유용하게 사용될 수 있다. 하지만 인간이 주고받는 언어의 입력과 출력 크기는 상황에 따라, 사람에 따라 매우 상이하다. 따라서 위에서 언급된 모델들로는 인간의 언어를 학습시키는 것에 어려움이 있다. 이러한 문제점을 해결하기 위해 고안된 모델이 오늘 설명할 seq2seq모델이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/',\n",
                            "     'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
                            "     'text': '1. Introduction pre-trained된 BERT모델을 downstream NLP task에 사용된다. 일반적으로 BERT는 unsupervised learning으로 학습되지만 다른 task에 이용하기 위해 supervised learning으로도 학습하여 사용될 수도 있다. supervised learning으로 학습된 BERT가 GLUE benchmark의 11개 task에서 기존의 state-of-the-art를 뛰어넘었다고 한다. 그런데 BERT가 왜 잘 되는지 아직 잘 모른다고 한다. research question: BERT의 각 layer에서 학습되는 representation이 무엇인지 밝힐 수 있을까?(각 layer에서 자연어의 어떤 특성이 학습되는지 밝힐 수 있을까?) 위와 같은 질문에 대한 답을 찾기 위해 본 논문에서는 아래와 같은 네 가지 실험을 한다. 이 실험들은 Goldberg(2019)연구를 기반으로 하여 각 layer에서 학습한 representation에 대한 평가를 진행한다. 1) 각 layer가 phrase정보를 얼마나 잘 포착했는지 실험 2) BERT가 자연어의 hierarchical한 속성을 잘 포착했는지 실험. Conneau(2018)연구를 기반으로 각 layer에서 surface, syntactics, semantics feature를 얼마나 잘 캡쳐하는지 실험. 3) 주어-술어 수일치와 같은 task가 있을 때 long-distance dependency에 대해 BERT가 task를 얼마나 잘 수행하는지 실험. 4) compositional structure를 test하여 BERT가 tree구조를 얼마나 잘 포착하는지 실험. 2. BERT'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
                            "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
                            "     'text': 'Illustrated by the author 여러 번 수행한 attention의 결과들(attention weight들)은 concatenate한 후 가중치 행렬 $W^{o}$와 내적하면 Multi-head attention의 최종 결과 값이 나온다. $W^{o}$도 다른 가중치들과 같이 모델과 함께 학습된다. \\\\begin{matrix} \\\\text{Attention}(Q, K, V) =\\\\text{Concat}({head}_ {1}, \\\\dots, {head}_ {h}){W}^{o} \\\\ where \\\\ {head}_ {i}=\\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\end{matrix} Illustrated by the author'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
                            "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
                            "     'text': 'Batch Size : 모델이 학습할 때 한 번에 처리하는 데이터 샘플의 개수이다. Batch Size가 작으면 한 번에 적은 양의 데이터를 처리한다. 메모리 사용량은 적지만 느리게 학습될 가능성이 높다. Batch Size가 크면 한 번에 많은 데이터를 처리한다. 학습이 빠를 수 있지만 메모리 사용이 많다는 문제가 있다. Epochs : 모델이 전체 데이터셋을 보는 횟수이다. Warm-up Steps : 훈련 초기에 Learning Rate 점진적으로 증가시켜 안정성을 높이는 요소이다. 2.3 Regularization Techniques 모델 학습 시 overfitting(과적합) 방지를 위해 몇 가지 정규화 기법이 사용된다. Dropout : 학습 시 일 neuron을 랜덤하게 비활성화하는 기법. 이를 통해 특정 뉴런에 모델이 너무 의존하지 않도록 할 수 있고, 다양한 neuron 조합을 학습하게 되어 모델의 일반화 능력을 향상시킬 수 있다. 예를 들어 Dropout 비율이 0.5라면, 학습 중 각 학습 단계마다 neuron의 절반을 무작위로 비활성화한다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/',\n",
                            "     'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
                            "     'text': 'RC-500: low-resource setting된 RCT data. 18040개의 RCT data 중 500개만 labeled data로 사용하고 나머지는 unlabeled data로 TAPT학습에 사용한다. (동일한 dataset에서 빼 온 것이기 때문에 동일한 task distribution에 속한다) HYPERPARTISAN: 5000개의 unlabeled data를 TAPT에 사용 IMDB: task annotator가 50000개의 unlabeled data를 직접 수집 2) Results'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/',\n",
                            "     'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
                            "     'text': '1. 우선 하나의 center word에 대한 context words의 one-hot-vectors를 만든다: \\\\begin{matrix} (x^{ (c-m) },x^{ (c-m+1) },…,x^{ (c-1) },x^{ (c+1) },…x^{ (c+m-1) },x^{ (c+m) })\\\\in\\\\mathbb{R}^{|V|} \\\\end{matrix} 2. 해당 모델의 파라미터는 input layer에서 hidden layer로 넘어가는 matrix $W$와 hidden layer에서 output layer로 넘어가는 matrix $W’$가 있다: \\\\begin{matrix} \\\\mathbf{W}\\\\in\\\\mathbb{R}^{|V|\\\\times N},~\\\\mathbf{W}^{\\\\prime}\\\\in\\\\mathbb{R}^{N\\\\times |V|} \\\\end{matrix} 위에서 만든 one-hot-vectors와 파라미터를 내적하여 context에 대한 embedded word vectors를 얻는다($W$는 input words에 대한 $n$차원의 embedding된 단어 vector들의 집합이다.): \\\\begin{matrix} (v_{ (c-m) }=\\\\mathbf{W}x^{ (c-m) },v_{ (c-m+1) }=\\\\mathbf{W}x^{ (c-m+1) }…,v_{ (c+m) }=\\\\mathbf{W}x^{ (c+m) })\\\\in\\\\mathbb{R}^n \\\\end{matrix} Input의 형태가 one-hot-vector이니 input과 파라미터의 내적은 아래 그림과 같이 파라미터에서 각 단어에 대한 행(column)을 look up해 오는 것이다. 즉, 파라미터와 내적하여 embedding된 vector를 불러오는 것이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/',\n",
                            "     'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
                            "     'text': 'Relative positional embedding에 대한 자세한 내용은 https://medium.com/@init/how-self-attention-with-relative-position-representations-works-28173b8c245a 여기에 잘 정리되어 있다. def relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128): ret = 0 n = -relative_position if bidirectional: num_buckets //= 2 ret += (n < 0).to(torch.long) * num_buckets n = torch.abs(n) else: n = torch.max(n, torch.zeros_like(n)) # now n is in the range (0, inf) # half of the buckets are for exact increments in positions max_exact = num_buckets // 2 is_small = n < max_exact # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance val_if_large = max_exact + ( torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact) ).to(torch.long) val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1)) ret += torch.where(is_small, n, val_if_large) return ret T5는 text-to-text framework를 기반으로 pre-training과 fine-tuning을 수행한다. T5의 text-to-text는 GPT3의 task description-prompt와 유사하다. input sentence 뒤에 task description이 prefix로 붙어 이것이 하나의 input text로 모델에 들어가 task의 정답을 text로 내놓는 것이다. input : task description(task 정보) + input sentence output : text T5 task 처리 방식'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/',\n",
                            "     'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
                            "     'text': '각 Modality별로 별도의 Transformer를 사용하고, Modality별 Q(Query) embedding을 cross-stream 방식으로 교환 및 교체하는 방식이다. 각 Modality의 최종 Embedding을 산출할 때 다른 Modality를 반영하면서도 계산 복잡도는 증가되지 않는다는 장점을 가진다. 하지만 각 Modality가 각각의 Transformer에 입력되기 전 cross-modal attention을 수행하지 못하여 전체 context를 놓칠 수 있다는 단점이 있다. 이 방법론은 VilBERT에서 처음 제안되었다. 해당 논문에서는 two-stream cross attention 은 modality 간의 interaction을 수행하긴 하지만 각 modality 내 자체 context에 대한 self-attention은 수행하지 않는다고 밝혔다. Multimodal Learning with Transformers:A Survey 3.2.6 cross-attention to concatenation Cross-attention의 단점을 보완하기 위해 나온 방법으로, Cross-attention에 Hierarchical 구조를 추가하여 global context를 반영할 수 있도록 만든 방법이다. 이는 Cross-attention을 통해 산출된 Embedding을 concat한 이후 또다른 Transformer layer에 입력한다. Multimodal Learning with Transformers:A Survey + Network Architectures 지금까지 알아본 multimodal Transformer는 Network Architecture와 interaction timing을 기준으로 아래 그림과 같이 분류할 수 있다. Multimodal Learning with Transformers:A Survey(edited by author) 4. Multimodal Pretraining task Taxonomy Multimodal Pretraining에 사용되는 task의 종류는 아래 표와 같이 매우 다양하다. task들은 크게 Masking, Describing, Matching, Ordering로 분류된다. Multimodal Learning with Transformers:A Survey(edited by author) 5. Challenges and Designs 5.1. Fusion'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/',\n",
                            "     'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
                            "     'text': 'TF-IDF(Term Frequency - Inverse Document Frequency) 문서에 등장한 단어를 단순히 count하는 방법이 아니라 다른 문서에 비해 더 많이 나온 단어를 알아보기 위한 방법이다. 다른 문서보다 특정 문서에서 더 많이 등장한 단어가 해당 문서의 핵심어일 가능성이 높다는 가정 하에 만들어진 방법이다. TF-IDF는 Term Frequency * Inverse Document Frequency로 계산된다. Term Frequency (앞서 다룬 DTM이랑 같다. 특정 문서에 나온 단어들의 빈도수), Inverse Document Frequency({전체문서 수}/{특정 단어가 들어있는 문서의 개수 + 1}에 로그를 씌워준 것) scikit-learn에서 tfidfvectorizer를 사용하여 구현할 수 있다. from sklearn.feature_extraction.text import TfidfVectorizer tfidf1 = TfidfVectorizer().fit(doc1) # doc1에 대해 tf-idf를 fit시킨다. print(tfidf1.transform(doc1).toarray()) print(tfidf1.vocabulary_)'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/',\n",
                            "     'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
                            "     'text': '이렇게 $L(\\\\theta)$를 최대화하는 학습 과정은 positive sample 단어쌍의 유사도는 높이고 negative sample 단어쌍의 유사도는 낮추는 방향으로 학습이 진행된다. 결국 FastText도 Skip-gram과 같이 center word와 주변 단어 쌍이 positive sample에 속하는지 negative sample에 속하는지 이진분류(binary classification)하는 과정에서 학습이 진행되고 학습을 통해 도출된 word embedding vector는 유사성에 관한 정보를 지니게 되는 것이다. Reference'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/',\n",
                            "     'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
                            "     'text': '- Task(a) Task(a)는 두 문장간의 상관관계를 파악하는 과제이다. 예를 들어 앞 문장과 뒷 문장의 흐름이 자연스러운지, 두 문장의 의미가 유사한지를 파악하는 것이다. 해당 task에 사용되는 dataset은 SWAG(The Situations With Adversarial Generations)와 GLUE dataset에 속하는 MNLI(Multi-Genre Natural Language Inference), QQP(Quora Question Pairs), QNLI(Question Natural Language Inference), STS-B(The Semantic Textual Similarity Benchmark), MRPC(Microsoft Research Paraphrase Corpus), RET(Recognizing Textual Entailment)이다. SWAG : 현재 문장 다음에 이어질 자연스러운 문장을 선택하기 위한 dataset MNLI : 현재 문장 다음에 이어지는 문장이 문맥상 이어지는 문장인지, 반대되는 문장인지, 상관 없는 문장인지 분류하기 위한 dataset QQP : 두 질문이 의미상 같은지 다른지 분류하기 위한 dataset QNLI : 질의응답 dataset STS-B : 두 문장의 유사성을 파악하기 위한 dataset MRPC : 뉴스의 내용과 사람이 만든 문장이 의미상 같은 문장인지 비교를 위한 dataset RET : MNLI와 유사하나 상대적으로 훨씬 적은 학습 dataset - Taks(b) Taks(b)는 input으로 하나의 문장을 입력하고 분류하는 것이다. 예를 들어 영화 리뷰 데이터를 입력한 후 해당 문장이 긍정인지 부정인지 혹은 중립인지를 판별하는 것이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/',\n",
                            "     'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
                            "     'text': '2. Related Work language representation모델이 문법적 지식을 평가하지 못하고 있다. 최근 연구들에서도 probing task, Minimal pair, Boolean acceptability judgment 등으로 평가를 시도하고 있지만 모델들의 직접적인 비교는 하지 목하고 있다. 따라서 본 논문에서는 NPI를 통해 BERT와 같은 language representation모델을 평가해보고자 한다. 2.1 Evaluating Sentence Encoders Boolean classification task, Minimal pair, probing task는 BERT의 encoder 부분을 평가하는 방법들이다. Boolean classification task를 통해 모델의 grammatical knowledge encoding 능력을 평가할 수 있다. 이 task의 목적은 입력된 단일 문장의 acceptability 예측하는 것이다. Minimal pair를 통해 언어학적 acceptability와 하나의 요소가 다른 두 문장쌍을 비교하여 모델이 single grammatical feature를 포착하는지 판단한다. Probing task는 모델의 embedding이 tense, voice, sentence length, morphological number와 같은 syntactic, surface feature들을 encoding하는지를 확인하기 위해 사용된다. 2.2 Negative Polarity Items NPI는 any처럼 부정적인 문장에서 받아들일 수 있는 단어이다. NPI는 licensor라는 환경이 구축되어야 사용될 수 있다. licensor는 일정의 언어적 환경으로, NPI를 쓰는 부정적인 환경이다. 그리고 Scope는 licensor가 미치는 영향의 범위이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/LSA(Latent-Sematic-Analysis)/',\n",
                            "     'source_title': 'LSA(Latent Sematic Analysis)\\n      03 Nov 2019',\n",
                            "     'text': '절단된 SVD(truncated SVD)수행을 위해 우선 $A$의 고유 값 개수($r$)보다 작은 임의의 $k$를 설정한 후, 대각 행렬 $\\\\Sigma$에서 가장 큰 $k$개의 값을 제외한 나머지를 제거하여 $Σ_k$를 만든다. 그리고 행렬$U$와 $V$에서 이와 대응하는 부분만 남겨 $U_k $ 와 $V_k\\u200b$를 만들어 모두 곱하면 행렬$A$와 근사하지만 차원이 줄어든 $A_k$가 만들어진다. 아래 그림을 참고하면 이해가 빠를 것이다. truncated SVD 결과가 갖는 의미에 대해 살펴보도록 하겠다. $U_kΣ_k{ V_k }^{ T }$로 도출된 행렬은 단어와 문서 간의 유사도를 나타내고, $U_kΣ_k{ V_k }^{ T }$에 ${ U_k }^{ T }$를 곱해 $Σ_k{ V_k }^{ T }$를 구하면 결과로 도출된 행렬의 행(colunm)은 문서 간의 유사도를 의미하게 된다. 그리고 $U_kΣ_k{ V_k }^{ T }$에 $V_k$를 곱하여 $U_kΣ_k$의 결과를 구하게 되면 도출된 행렬의 열(row)는 단어 간의 유사도를 의미한다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/',\n",
                            "     'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
                            "     'text': '또한 같은 domain에 속한 다른 task간의 knowledge transfer가 되는지 확인한 실험도 진행하였다. 예를 들어 RCT unlabeled data로 학습한 LM(RCT에 대한 TAPT)으로 CHEMPROT labeled data에 대한 fine-tuning을 진행하는 방식의 실험인데 이러한 방법론을 본 논문에서 Transfer-TAPT라고 칭한다. Table 6에 해당 실험에 대한 결과가 정리되어 있다. 전반적으로 성능이 좋지 않은데, 이는 동일한 domain에 속해도 각 task의 data distribution이 다를 수 있다는 것을 보여준다. 더 나아가 이 실험의 결과를 통해 왜 domain 지식에 대한 adapting만으로는 충분하지 않은지, 그리고 왜 DAPT 이후에 TAPT를 수행해야 하는지도 설명할 수 있다. 4. Augmenting Training Data for Task-Adaptive Pretraining 더 많은 task distribution에 속한 unlabeled data가 더 많은 상황에 대한 결과를 확인하기 위한 실험도 진행하였다. task data augmentation 방식으로는 task와 관련된 unlabeled data를 사람이 선별하는 방식과 자동으로 고르는 방식이 제안되었다. 4.1 Human Curated-TAPT 명확한 source로부터 사람이 직접 다량의 unlabeled data를 수집하여 task training data distribution과 유사한 분포의 dataset을 생성하여 TAPT를 수행하는 것을 Human Curated-TAPT라 부른다. 1) Data (Table 2 참고)'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/',\n",
                            "     'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
                            "     'text': '본 연구의 probing model들의 실패를 더 잘 이해하기 위해 아래 표와 같이 또 다른 실험을 진행하였다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/',\n",
                            "     'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
                            "     'text': 'parse tree에서의 edge는 parse tree에 있는 단어들의 depth에 의해 결정된다. governance relationship(지배관계)에서 더 깊은 node는 지배 단어이다. parse tree에서의 depth는 norm과 같다. 이는 tree의 node를 결정한다. 이 tree depth norm을 ${\\\\lVert w_i \\\\rVert}$라고 한다. 우리는 squared L2 vector norm ${\\\\lVert Bh_i \\\\rVert}_{2}^{2}$ 으로 tree depth norm이 encoding되는 선형 변형이 존재한다는 것을 전제한다. 그리고 거리 가설처럼 depth norm가설은 best-approxiamted된 선형 변형을 찾을 수 있다. \\\\begin{matrix} \\\\underset{B}{\\\\min}\\\\sum_{\\\\ell}\\\\frac{1}{\\\\left\\\\vert s_{\\\\ell} \\\\right\\\\vert}\\\\sum_i(\\\\lVert w_i \\\\rVert-{\\\\lVert Bh_i \\\\rVert}^2) \\\\end{matrix} 3.2 Properties of the structural probe squared distance space가 syntax tree distance를 encoding한 representation space에 내적이 존재한다는 주장을 테스트할 수 있다. 이는 모델이 단어가 다른 단어에 의해 지배되는 것을 encoding할 뿐만 아니라 syntax tree에서의 단어들 간의 proximity를 encoding한다는 것을 의미한다. 3.3 Tree depth structural probes 단어 $w_i$에 대한 parse depth $\\\\lVert w_i \\\\rVert$는 parse tree에서 $w_i$의 edge와 tree의 root에 의해 결정된다. 본 논문에서는 tree norm을 encoding한 word representation space에 squared norm이 존재하는지 확인하고자 한다. 결과적으로 norm이 distance와 크게 다르지 않고 depth가 norm으로 표현된다고 한다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
                            "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
                            "     'text': 'RNN모델이 자연어처리 과제에서 보편적으로 활용될 수 있었던 이유는 해당 모델이 단어의 위치 및 순서 정보를 반영한다는 특징 때문이었다. 하지만 RNN을 전혀 사용하지 않는 Transformer는 따로 위치 정보를 반영해주어야 한다. Positional Encoding은 위와 같은 이유로 Embedding된 encoder와 decoder의 입력 값에 sinusoid function을 사용하여 positional encoding값을 더해 위치정보를 주는 과정이다. 단어별(벡터별) positional encoding수행 과정을 그림으로 표현하면 다음과 같다:'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/',\n",
                            "     'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
                            "     'text': '학습에 사용되는 데이터는 OCR을 통해 추출한 text token과 그에 대한 bounding box가 포함된 데이터이다. 해당 모델은 기존의 Multimodal Model들이 시각 정보를 처리할 때 image encoder를 사용하는 것과는 달리 문서 내 시각 정보(layout 구조)를 이해할 때 OCR을 통해 얻어진 bounding box 정보를 사용한다. (image encoder를 사용하지 않기 때문에 모델 크기가 타 Multimodal Model에 비하여 작고, 이에 따라 추론 시간도 단축된다.) 해당 모델은 text semantic과 spatial layout 간의 관계를 포착할 때 확장된 attention mecanism을 도입한다. 본 모델은 문서 내 text 와 공간적(spatial) 정보를 각각 따로 처리하기 위해 두 modality 간의 cross-alignment를 포착할 때 classical transformer의 self-attention mechanism을 분해하여 사용한다. 구체적으로, 각 modality에 대한 attention score 뿐만 아니라 두 modality간의 관계를 포착하는 attention score도 계산한다. 다양한 layout과 문서 내 시각적 정보를 잘 파악하기 위해 사전 학습 과정에서 infill text segment task를 학습한다. infill text segment task는 다양한 layout을 지닌 문서에서 이전 text 정보가 현재 text와 관련 없을 가능성이 있기 때문에 이러한 경우를 파악하기에 적합한 과제이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
                            "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
                            "     'text': '위에서 언급했듯이 positional encoding은 sine과 cosine함수를 통해 위치정보를 가진 encoding값을 만들어 embedding 벡터와 더하는 과정이다. 이를 위한 sinusoid function은 다음과 같이 정의된다: \\\\begin{matrix} PE_{(pos,2i)}=\\\\sin(pos/10000^{2i/d_{\\\\text{model}}}) PE_{(pos,2i+1)}=\\\\cos(pos/10000^{2i/d_{\\\\text{model}}}) \\\\end{matrix} sinusoid function을 사용하여 positional encoding을 해주는 이유는 인코딩 값이 -1부터 1사이의 값이 나오게 되고, 학습데이터보다 긴 문장이 입력돼도 오류 없이 상대적인 encoding 값을 줄 수 있다는 장점이 있기 때문이다. 3. Multi-Head Attention Embedding과 positional encoding과정을 거친 후에는 본격적으로 encoder와 decoder에 입력이 되어 attention layer에 들어간다. 해당 모델에서는 Multi-head attention을 사용했는데 말 그대로 head가 여러개인 attention을 쓴다는 것이다. Transformer에서는 기존 seq2seq 모델에서 사용되던 source-target attention과는 다른 종류의 attention을 사용한다.'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
                            "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
                            "     'text': 'Forward Pass (순전파): Backpropagation을 위해서는 예측값이 있어야 하니까 우선 순전파로 모델의 예측값을 구한다. Loss 계산: loss function을 통해 예측값과 실제값 간의 오를 계산한다. (cross-entropy, perplexity loss 등을 사용) (오차함수(error function) = 손실함수(loss function) = 비용함수(cost function)) Backward Pass (역전파): 손실함수를 통해 산출된 전체 에러값($E$)을 출력층->입력층으로 전파하며 각 layer마다의 weight가 전체 에러값 $E$에 얼마나 영향을 미쳤는지 그 기여도를 구하하고 그걸 경사하강법에 대입하여 가중치를 업데이트 한다. 1) 산출된 오차값을 출력층->입력층으로 전파하여 각 layer의 weight가 전체 오차에 얼마나 영향을 주었는지 그 기여도 Chain Rule을 통해 구한다. 이 값은 해당 layer의 weight에 대한 기울기이다. 2) 가중치 업데이트 공식에 산출된 기울기와 Learning Rate를 넣어 가중치를 업데이트한다. 이때 사용되는 가중치 업데이트 공식이 경사하강법이다. Learning Rate는 얼마나 빨리 학습시킬지 정하는 것인데 일반적으로 0.1보다 낮은 값으로 직접 설정한다. 2.1 Training Strategies'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/',\n",
                            "     'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
                            "     'text': '5. Pretrained Contextualizer Comparison: What features of language do these vectors capture, and what do they miss? 첫 번째 실험은 앞서 제시한 문제 중 첫 번째 문제와 관련되며, 어떤 언어학적 지식을 습득하고 어떤 것을 습득하지 못하는가에 대한 실험이다. 5.1 Experimental Setup 본 실험에 사용되는 probing model들은 각 contextualizer의 개별 layer를 통해 생성된 representation으로 훈련되었다. probing model들 간의 비교 외에도 noncontextual vector로 학습된 GloVe와 pretraining을 사용하지 않은 각 task의 이전 state of the art점수와의 비교도 진행된다. 5.2 Results and Discussion'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/08/01/llama3_1/',\n",
                            "     'source_title': 'LLaMa 3.1\\n      01 Aug 2024',\n",
                            "     'text': 'Evaluation LLaMa 3.1는 다양한 언어에 대한 150개 이상의 benchmark에 대해 성능 평가를 진행하였고, real-world scenario를 통한 human evaluation도 진행하였다. 아래 표 이번 LLaMa 3.1의 flagship model인 405B 모델의 실험 결과이다. GPT-4, GPT-4o, Claude 3.5 Sonnet과 비교하였을 때 경쟁력있는 성능을 보이는 것으로 확인되었다. 8B와 70B또한 비슷한 크기의 모델들과 비요하였을 때 경쟁력있는 성능을 보였다. Mixture-of-Agents Enhances Large Language Model Capabilities Mixture-of-Agents Enhances Large Language Model Capabilities Mixture-of-Agents Enhances Large Language Model Capabilities Model Architecture Mixture-of-Agents Enhances Large Language Model Capabilities'},\n",
                            "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/',\n",
                            "     'source_title': 'Mistral 7B\\n      04 Dec 2023',\n",
                            "     'text': '2. Sliding window to speed-up inference and reduce memory pressure attention 연산 횟수는 sequence length에 대해 이차적(quadratic)이고 memory pressure은 sequence length에 대해 선형적이다. 이로 인해 추론 시 cache 가용성이 감소하기 때문에 데이터 처리량은 줄고 시간은 더 지연된다. 이 문제를 완화하기 위해 Mistal은 각 token이 window size만큼의 과거 토큰에만 attention을 하는 Sliding Window Attention을 사용한다. 아래 그림은 window size가 3인 경우이다. 해당 방법론의 특이 사항은 sliding window 범위 밖에 있는 token도 next word prediction에 영향을 미친다는 점이다. 각 attention layer에서 정보는 최대 window size 만큼의 token만 전달될 수 있다. 따라서 2개의 attention layer를 거친 후에는 2*window size만큼의 정보가 전달될 수 있다. 예를 들어 sequence length가 16k이고 sliding window size가 4k일 때 4 layer를 거친 후에는 정보가 전체 sequence length로 전달된다. 이와 같이 해당 방법론이 sliding window 밖의 정보도 전달하지만 sequence 길이가 너무 길어지면 모델이 더 이상 full context를 전부 사용하진 않는 현상이 관찰되었다고 한다 3. Rolling buffer cache Mistral에는 rolling buffer cache가 적용되었다. cache는 W(window size)로 고정되고 (key, value)값을 cache position(i%W) 내의 position i에 저장한다. position i가 W보다 크면 과거 cache의 value들이 덮어씌워진다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/',\n",
                            "     'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
                            "     'text': 'Table 7를 보면 Curated-TAPT, TAPT, DAPT+TAPT에 대한 실험 결과가 비교할 수 있다. Curated-TAPT 결과가 이전에 실험 결과가 가장 좋았던 DAPT+TAPT보다 좋게 나온 것을 확인할 수 있다. 그리고 DAPT이후 Curated-TAPT를수행한 결과가 모든 task에서 가장 높은 성능을 보였다. 이러한 결과는 task distribution에 속한 다량의 curating data가 end-task 성능에 좋은 영향을 미친다는 것을 시사한다. 4.2 Automated Data Selection for TAPT 두 번째 증강 방법은 다량의 in-domain corpus에서 task distribution과 연관된 unlabeled text를 찾아 다량의 unlabeled data를 자동으로 생성하는 방식이다. 조금 더 구체적으로 말하자면, embedding space 내에 task 와 domain이 공유하는 space에서 domain data 중 task-relevant data를 찾아 후보를 선별하여 data를 증강시키는 것이다. 이 실험에서 중요한 것은 다량의 문장을 embedding하기 위해 embedding 모델은 가벼우면서 빨라야 한다는 것이다. 저자는 이러한 조건을 충족시키는 모델인 lightweight bag-of-words language model인 VAMPIRE를 사용하여 (i) task sentence 근처에 있는 k개의 domain data 50/100/150개(kNN-TAPT), (ii) 랜덤하게 선별한 data 50개(RAND-TAPT)를 뽑았다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/',\n",
                            "     'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
                            "     'text': 'NLP Basics 실습 코드: https://github.com/finddme/Finddme_Blog_Code/blob/master/NLP_Code/Sentimental_Categorize1.ipynb 홍종선(2009)에서는 의미적 차원과 표현 구조를 검토하여 감정동사들을 총19유형(감동, 감사, 기쁨, 고뇌, 고독, 놀람, 동정, 두려움, 미안함, 분노, 상쾌함,수용, 수치심, 슬픔, 욕구, 이완, 허무, 혐오, 후회)으로 분류했다. 표현 구조를고려하여 분류된 감정동사들의 벡터 값이 유사할 것이라는 가정 하에 Word2Vec을 활용하여 이를 검토해보고자 한다. 데이터는 AI 허브에서 제공하는 감성분석 데이터에서 content만 따로 추출하여 데이터로 사용한다. import codecs from bs4 import BeautifulSoup from konlpy.tag import Okt from gensim.models import word2vec text = codecs.open(\"sent.content.txt\", \"r\", encoding = \"utf-8\") f = open(\"sent.content.txt\", \"r\", encoding=\"utf-8\") print(f.read(20)) 곧 만기 은퇴 앞두다 있다 노후 준비 word_dict = {} lines = f.read().split(\"\\\\n\") print(lines[10]) 친구 내 자다 되다 건 다 너 덕분 고맙다 문자 하다 통 넣다 morph_analyzer = Okt() morph_analyzer1 = Okt() for line in lines: morph_analysed = morph_analyzer.pos(line) for word in morph_analysed: if word[1] == \"Noun\": if not (word[0] in word_dict): word_dict[word[0]] = 0 word_dict[word[0]] += 1'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/',\n",
                            "     'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
                            "     'text': 'NPI는 자연언어에서 개념적으로 정의할 수는 있지만 나타나는 수많은 구성양상을 모두 설명하는 것은 불가능하다. 따라서 NPI는 언어학에서 매우 tricky한 주제이기 때문에 본 논문의 주제로 선정되었을 것이다. 2.3 CoLA(Corpus of Linguistic Acceptability) CoLA는 데이터셋이다. 본 논문은 CoLA를 기반으로 생성한 데이터를 직접 구현하여 사용한다. CoLA는 1만개 이상의 example sentence를 지니는 데이터셋이며, supervised acceptability classifier를 수행하기 위해 사용된다. 한국어 NPI는 영어보다 복잡하다. 왜냐하면 한국어의 NPI가 영어보다 많기 때문이다. 영어에서는 NPI가 명확한데 한국어에서는 이게 NPI인가 싶은 것들도 NPI인 경우가 많다. 그리고 한국어 NPI는 문맥상의 의미에 따라 여러 제약이 존재한다. 한국어 NPI를 연구하여 딥러닝 모델에 적용시킬 수 있다면 좋을텐데…'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/',\n",
                            "     'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
                            "     'text': '이 그림에서 보이는 동그라미들을 Neuron이라고 부른다. 딥러닝에서는 그림과 같은 구조 통해 Neuron들이 input data(x)의 features를 찾아낸다. 그림을 예로 들자면 위와 같은 구조를 통해 기계는 고양이에 대한 정보를 고양이의 얼굴, 꼬리, 발, 털 등으로 분해하여 각각의 Neuron에 담아 고양이에 대한 정보를 학습한다. 이처럼 딥러닝은 raw data(e.g., sound, characters of words)인 input data로부터 좋은 features를 자동으로 추출하여 학습하는데 이를 다른 말로 Representation learning이라고도 한다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) × Search'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/',\n",
                            "     'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
                            "     'text': 'https://nlp.stanford.edu/~johnhew/structural-probe.html#the-structural-probe https://github.com/john-hewitt/structural-probes × Search'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/',\n",
                            "     'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
                            "     'text': '본 논문에서는 Prompt based learning 기법 중 하나인 PET(Pattern-Exploiting Training)를 소개한다. PET은 BERT 계열의 모델들이 prompt를 활용하여 task를 푸는 방법론이다. 이 방법론은 in-context learning의 개념을 차용해서 Language Model이 특정 task를 수행할 수 있도록 PVP(Pattern Verbalizer Pair)를 사용하여 Pre-training Task와 동일한 형태, 즉 cloze-style phrase 형태로 문제(input)를 재정의하는(e.g. “the correct answer is __”와 같은 cloze question 붙임) 것이다.'},\n",
                            "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
                            "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
                            "     'text': '근본적인 차이이다.'}]}}}"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9712584e-9be8-4280-b6a6-98d1445c664a",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}